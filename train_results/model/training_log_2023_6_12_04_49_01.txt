2023-06-12 04:49:03.311424: Created Ð° model 
2023-06-12 04:49:03.312243: Total number of trainable parameters 20757084 
2023-06-12 04:49:04.582809: Total number of flops 118581626112 
2023-06-12 04:49:04.590301: Train dataset number of batch: 129 
2023-06-12 04:49:04.590407: Val dataset number of batch: 72 
2023-06-12 04:49:04.590455: Benchtest dataset number of batch: 25 
2023-06-12 04:49:04.590553: start training now! 
2023-06-12 04:49:04.590611: Batches per epoch:  129 
2023-06-12 04:49:09.410929: train Epoch: [0][  0/129]	Time  4.820 ( 4.820)	Data  3.858 ( 3.858)	Loss 8.5946e-01 (8.5946e-01) 
2023-06-12 04:49:10.244945: train Epoch: [0][  1/129]	Time  0.834 ( 2.827)	Data  0.001 ( 1.929)	Loss 8.6979e-01 (8.6462e-01) 
2023-06-12 04:49:12.573480: train Epoch: [0][  2/129]	Time  2.329 ( 2.661)	Data  1.498 ( 1.785)	Loss 8.6864e-01 (8.6596e-01) 
2023-06-12 04:49:13.409553: train Epoch: [0][  3/129]	Time  0.836 ( 2.205)	Data  0.001 ( 1.339)	Loss 8.1921e-01 (8.5427e-01) 
2023-06-12 04:49:15.865176: train Epoch: [0][  4/129]	Time  2.456 ( 2.255)	Data  1.623 ( 1.396)	Loss 8.5496e-01 (8.5441e-01) 
2023-06-12 04:49:16.700401: train Epoch: [0][  5/129]	Time  0.835 ( 2.018)	Data  0.001 ( 1.163)	Loss 8.1323e-01 (8.4755e-01) 
2023-06-12 04:49:19.193276: train Epoch: [0][  6/129]	Time  2.493 ( 2.086)	Data  1.661 ( 1.235)	Loss 8.2587e-01 (8.4445e-01) 
2023-06-12 04:49:20.026417: train Epoch: [0][  7/129]	Time  0.833 ( 1.929)	Data  0.001 ( 1.080)	Loss 8.2856e-01 (8.4246e-01) 
