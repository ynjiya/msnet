2023-06-12 04:51:05.261170: Created Ð° model 
2023-06-12 04:51:05.261982: Total number of trainable parameters 20757084 
2023-06-12 04:51:06.530667: Total number of flops 118581626112 
2023-06-12 04:51:06.538475: Train dataset number of batch: 193 
2023-06-12 04:51:06.538594: Val dataset number of batch: 72 
2023-06-12 04:51:06.538636: Benchtest dataset number of batch: 25 
2023-06-12 04:51:06.538745: start training now! 
2023-06-12 04:51:06.538800: Batches per epoch:  193 
2023-06-12 04:51:09.829460: train Epoch: [0][  0/193]	Time  3.290 ( 3.290)	Data  2.696 ( 2.696)	Loss 8.9327e-01 (8.9327e-01) 
2023-06-12 04:51:10.402307: train Epoch: [0][  1/193]	Time  0.573 ( 1.932)	Data  0.001 ( 1.349)	Loss 8.1307e-01 (8.5317e-01) 
2023-06-12 04:51:11.927333: train Epoch: [0][  2/193]	Time  1.525 ( 1.796)	Data  0.959 ( 1.219)	Loss 8.6419e-01 (8.5684e-01) 
2023-06-12 04:51:12.495358: train Epoch: [0][  3/193]	Time  0.568 ( 1.489)	Data  0.001 ( 0.914)	Loss 8.7218e-01 (8.6068e-01) 
2023-06-12 04:51:14.163913: train Epoch: [0][  4/193]	Time  1.669 ( 1.525)	Data  1.104 ( 0.952)	Loss 8.2121e-01 (8.5279e-01) 
2023-06-12 04:51:14.732184: train Epoch: [0][  5/193]	Time  0.568 ( 1.366)	Data  0.001 ( 0.794)	Loss 8.4303e-01 (8.5116e-01) 
2023-06-12 04:51:16.399897: train Epoch: [0][  6/193]	Time  1.668 ( 1.409)	Data  1.098 ( 0.837)	Loss 8.2658e-01 (8.4765e-01) 
2023-06-12 04:51:16.971657: train Epoch: [0][  7/193]	Time  0.572 ( 1.304)	Data  0.001 ( 0.733)	Loss 8.0209e-01 (8.4195e-01) 
2023-06-12 04:51:18.641495: train Epoch: [0][  8/193]	Time  1.670 ( 1.345)	Data  1.101 ( 0.773)	Loss 8.0797e-01 (8.3818e-01) 
2023-06-12 04:51:19.215122: train Epoch: [0][  9/193]	Time  0.574 ( 1.268)	Data  0.001 ( 0.696)	Loss 8.1694e-01 (8.3605e-01) 
2023-06-12 04:51:20.837943: train Epoch: [0][ 10/193]	Time  1.623 ( 1.300)	Data  1.054 ( 0.729)	Loss 7.6190e-01 (8.2931e-01) 
2023-06-12 04:51:21.409684: train Epoch: [0][ 11/193]	Time  0.572 ( 1.239)	Data  0.001 ( 0.668)	Loss 7.7348e-01 (8.2466e-01) 
2023-06-12 04:51:22.953518: train Epoch: [0][ 12/193]	Time  1.544 ( 1.263)	Data  0.980 ( 0.692)	Loss 7.4920e-01 (8.1886e-01) 
2023-06-12 04:51:23.518595: train Epoch: [0][ 13/193]	Time  0.565 ( 1.213)	Data  0.001 ( 0.643)	Loss 7.4785e-01 (8.1378e-01) 
2023-06-12 04:51:25.261481: train Epoch: [0][ 14/193]	Time  1.743 ( 1.248)	Data  1.178 ( 0.678)	Loss 7.6024e-01 (8.1021e-01) 
2023-06-12 04:51:25.828039: train Epoch: [0][ 15/193]	Time  0.567 ( 1.206)	Data  0.001 ( 0.636)	Loss 7.6287e-01 (8.0726e-01) 
2023-06-12 04:51:27.387704: train Epoch: [0][ 16/193]	Time  1.560 ( 1.226)	Data  0.995 ( 0.657)	Loss 7.4538e-01 (8.0362e-01) 
2023-06-12 04:51:27.954311: train Epoch: [0][ 17/193]	Time  0.567 ( 1.190)	Data  0.001 ( 0.621)	Loss 7.1083e-01 (7.9846e-01) 
2023-06-12 04:51:29.595667: train Epoch: [0][ 18/193]	Time  1.641 ( 1.214)	Data  1.077 ( 0.645)	Loss 7.0851e-01 (7.9373e-01) 
2023-06-12 04:51:30.162207: train Epoch: [0][ 19/193]	Time  0.567 ( 1.181)	Data  0.001 ( 0.613)	Loss 7.4482e-01 (7.9128e-01) 
2023-06-12 04:51:31.782686: train Epoch: [0][ 20/193]	Time  1.620 ( 1.202)	Data  1.056 ( 0.634)	Loss 7.4394e-01 (7.8903e-01) 
2023-06-12 04:51:32.348352: train Epoch: [0][ 21/193]	Time  0.566 ( 1.173)	Data  0.001 ( 0.605)	Loss 7.0914e-01 (7.8540e-01) 
2023-06-12 04:51:33.932231: train Epoch: [0][ 22/193]	Time  1.584 ( 1.191)	Data  1.020 ( 0.623)	Loss 7.0499e-01 (7.8190e-01) 
2023-06-12 04:51:34.497328: train Epoch: [0][ 23/193]	Time  0.565 ( 1.165)	Data  0.001 ( 0.597)	Loss 7.3587e-01 (7.7998e-01) 
