2023-05-24 23:49:41.736484: Created model 
2023-05-24 23:49:41.737808: total number of trainable parameters 20757084 
2023-05-24 23:49:41.741084: <bound method EDiceLoss.metric of EDiceLoss()> 
2023-05-24 23:49:41.746546: Train dataset number of batch: 24 
2023-05-24 23:49:41.746732: Val dataset number of batch: 9 
2023-05-24 23:49:41.746774: Bench Test dataset number of batch: 3 
2023-05-24 23:49:41.746897: start training now! 
2023-05-24 23:49:41.746948: Batches per epoch:  24 
2023-05-24 23:49:47.072587: train Epoch: [0][ 0/24]	Time  5.325 ( 5.325)	Data  3.310 ( 3.310)	Loss 8.2326e-01 (8.2326e-01) 
2023-05-24 23:49:48.149944: train Epoch: [0][ 1/24]	Time  1.077 ( 3.201)	Data  0.001 ( 1.656)	Loss 8.4930e-01 (8.3628e-01) 
2023-05-24 23:49:49.090103: train Epoch: [0][ 2/24]	Time  0.940 ( 2.448)	Data  0.001 ( 1.104)	Loss 8.7202e-01 (8.4819e-01) 
2023-05-24 23:49:50.050975: train Epoch: [0][ 3/24]	Time  0.961 ( 2.076)	Data  0.001 ( 0.828)	Loss 8.5802e-01 (8.5065e-01) 
2023-05-24 23:49:51.539306: train Epoch: [0][ 4/24]	Time  1.488 ( 1.958)	Data  0.439 ( 0.750)	Loss 8.4663e-01 (8.4984e-01) 
2023-05-24 23:49:52.646390: train Epoch: [0][ 5/24]	Time  1.107 ( 1.817)	Data  0.001 ( 0.626)	Loss 8.6299e-01 (8.5203e-01) 
2023-05-24 23:49:54.353673: train Epoch: [0][ 6/24]	Time  1.707 ( 1.801)	Data  0.532 ( 0.612)	Loss 8.2653e-01 (8.4839e-01) 
2023-05-24 23:49:55.359523: train Epoch: [0][ 7/24]	Time  1.006 ( 1.702)	Data  0.001 ( 0.536)	Loss 8.0384e-01 (8.4282e-01) 
2023-05-24 23:49:57.060590: train Epoch: [0][ 8/24]	Time  1.701 ( 1.701)	Data  0.452 ( 0.527)	Loss 8.4356e-01 (8.4290e-01) 
2023-05-24 23:49:58.313094: train Epoch: [0][ 9/24]	Time  1.253 ( 1.657)	Data  0.001 ( 0.474)	Loss 8.5234e-01 (8.4385e-01) 
2023-05-24 23:49:59.615916: train Epoch: [0][10/24]	Time  1.303 ( 1.624)	Data  0.183 ( 0.448)	Loss 7.7248e-01 (8.3736e-01) 
2023-05-24 23:50:00.883258: train Epoch: [0][11/24]	Time  1.267 ( 1.595)	Data  0.001 ( 0.410)	Loss 8.0121e-01 (8.3435e-01) 
2023-05-24 23:50:02.112920: train Epoch: [0][12/24]	Time  1.230 ( 1.567)	Data  0.227 ( 0.396)	Loss 7.8133e-01 (8.3027e-01) 
2023-05-24 23:50:03.222893: train Epoch: [0][13/24]	Time  1.110 ( 1.534)	Data  0.016 ( 0.369)	Loss 7.6445e-01 (8.2557e-01) 
2023-05-24 23:50:04.821344: train Epoch: [0][14/24]	Time  1.598 ( 1.538)	Data  0.661 ( 0.388)	Loss 7.9772e-01 (8.2371e-01) 
2023-05-24 23:50:05.983609: train Epoch: [0][15/24]	Time  1.162 ( 1.515)	Data  0.174 ( 0.375)	Loss 7.9342e-01 (8.2182e-01) 
2023-05-24 23:50:07.817377: train Epoch: [0][16/24]	Time  1.834 ( 1.534)	Data  0.547 ( 0.385)	Loss 7.6465e-01 (8.1846e-01) 
2023-05-24 23:50:08.844288: train Epoch: [0][17/24]	Time  1.027 ( 1.505)	Data  0.001 ( 0.364)	Loss 7.1388e-01 (8.1265e-01) 
2023-05-24 23:50:10.544554: train Epoch: [0][18/24]	Time  1.700 ( 1.516)	Data  0.448 ( 0.368)	Loss 7.5634e-01 (8.0968e-01) 
2023-05-24 23:50:11.717966: train Epoch: [0][19/24]	Time  1.173 ( 1.499)	Data  0.081 ( 0.354)	Loss 7.4996e-01 (8.0670e-01) 
2023-05-24 23:50:13.176466: train Epoch: [0][20/24]	Time  1.458 ( 1.497)	Data  0.326 ( 0.353)	Loss 7.3711e-01 (8.0338e-01) 
2023-05-24 23:50:14.638770: train Epoch: [0][21/24]	Time  1.462 ( 1.495)	Data  0.336 ( 0.352)	Loss 7.2509e-01 (7.9982e-01) 
2023-05-24 23:50:15.781366: train Epoch: [0][22/24]	Time  1.143 ( 1.480)	Data  0.163 ( 0.344)	Loss 7.3843e-01 (7.9716e-01) 
2023-05-24 23:50:16.952549: train Epoch: [0][23/24]	Time  1.171 ( 1.467)	Data  0.186 ( 0.337)	Loss 7.0605e-01 (7.9336e-01) 
2023-05-24 23:50:17.001686: Train Epoch done in 35.25474050804041 s 
2023-05-24 23:50:20.087816: val Epoch: [0][0/9]	Time  1.996 ( 1.996)	Data  1.756 ( 1.756)	Loss 7.8157e-01 (7.8157e-01) 
2023-05-24 23:50:20.368126: val Epoch: [0][1/9]	Time  0.280 ( 1.138)	Data  0.001 ( 0.879)	Loss 7.2970e-01 (7.5563e-01) 
2023-05-24 23:50:21.682196: val Epoch: [0][2/9]	Time  1.314 ( 1.197)	Data  0.936 ( 0.898)	Loss 6.8193e-01 (7.3107e-01) 
2023-05-24 23:50:22.118134: val Epoch: [0][3/9]	Time  0.436 ( 1.007)	Data  0.001 ( 0.673)	Loss 7.7180e-01 (7.4125e-01) 
2023-05-24 23:50:23.195339: val Epoch: [0][4/9]	Time  1.077 ( 1.021)	Data  0.633 ( 0.665)	Loss 7.6988e-01 (7.4698e-01) 
2023-05-24 23:50:23.448539: val Epoch: [0][5/9]	Time  0.253 ( 0.893)	Data  0.001 ( 0.555)	Loss 6.9952e-01 (7.3907e-01) 
2023-05-24 23:50:24.357330: val Epoch: [0][6/9]	Time  0.909 ( 0.895)	Data  0.629 ( 0.565)	Loss 7.7215e-01 (7.4379e-01) 
2023-05-24 23:50:24.743259: val Epoch: [0][7/9]	Time  0.386 ( 0.831)	Data  0.001 ( 0.495)	Loss 7.2571e-01 (7.4153e-01) 
2023-05-24 23:50:25.570788: val Epoch: [0][8/9]	Time  0.828 ( 0.831)	Data  0.523 ( 0.498)	Loss 7.1602e-01 (7.3870e-01) 
2023-05-24 23:50:25.869804: Epoch 0 :Val : ['ET : 0.00524051021784544', 'TC : 0.01011922862380743', 'WT : 0.0002348184207221493'] 
2023-05-24 23:50:25.872723: Epoch 0 :Val : ['ET : 0.00524051021784544', 'TC : 0.01011922862380743', 'WT : 0.0002348184207221493'] 
2023-05-24 23:50:25.875972: Saving the model with DSC 0.0051981862634420395 
2023-05-24 23:50:27.006892: Val epoch done in 10.005190997966565 s 
2023-05-24 23:50:27.016774: Batches per epoch:  24 
2023-05-24 23:50:31.308752: train Epoch: [1][ 0/24]	Time  4.291 ( 4.291)	Data  3.254 ( 3.254)	Loss 7.5361e-01 (7.5361e-01) 
2023-05-24 23:50:32.423098: train Epoch: [1][ 1/24]	Time  1.114 ( 2.703)	Data  0.001 ( 1.628)	Loss 7.6695e-01 (7.6028e-01) 
2023-05-24 23:50:34.118123: train Epoch: [1][ 2/24]	Time  1.695 ( 2.367)	Data  0.709 ( 1.321)	Loss 7.2552e-01 (7.4869e-01) 
2023-05-24 23:50:35.147859: train Epoch: [1][ 3/24]	Time  1.030 ( 2.033)	Data  0.001 ( 0.991)	Loss 6.7211e-01 (7.2955e-01) 
2023-05-24 23:50:37.190095: train Epoch: [1][ 4/24]	Time  2.042 ( 2.035)	Data  0.729 ( 0.939)	Loss 6.9429e-01 (7.2249e-01) 
2023-05-24 23:50:38.230325: train Epoch: [1][ 5/24]	Time  1.040 ( 1.869)	Data  0.001 ( 0.783)	Loss 7.0695e-01 (7.1990e-01) 
2023-05-24 23:50:40.170776: train Epoch: [1][ 6/24]	Time  1.940 ( 1.879)	Data  0.470 ( 0.738)	Loss 6.9093e-01 (7.1576e-01) 
2023-05-24 23:50:41.286717: train Epoch: [1][ 7/24]	Time  1.116 ( 1.784)	Data  0.001 ( 0.646)	Loss 7.0219e-01 (7.1407e-01) 
2023-05-24 23:50:42.665993: train Epoch: [1][ 8/24]	Time  1.379 ( 1.739)	Data  0.175 ( 0.594)	Loss 6.7317e-01 (7.0952e-01) 
2023-05-24 23:50:43.846906: train Epoch: [1][ 9/24]	Time  1.181 ( 1.683)	Data  0.001 ( 0.534)	Loss 6.5725e-01 (7.0430e-01) 
2023-05-24 23:50:45.442950: train Epoch: [1][10/24]	Time  1.596 ( 1.675)	Data  0.406 ( 0.523)	Loss 7.2394e-01 (7.0608e-01) 
2023-05-24 23:50:46.519892: train Epoch: [1][11/24]	Time  1.077 ( 1.625)	Data  0.001 ( 0.479)	Loss 7.1279e-01 (7.0664e-01) 
2023-05-24 23:50:48.062026: train Epoch: [1][12/24]	Time  1.542 ( 1.619)	Data  0.439 ( 0.476)	Loss 6.8257e-01 (7.0479e-01) 
2023-05-24 23:50:49.141269: train Epoch: [1][13/24]	Time  1.079 ( 1.580)	Data  0.001 ( 0.442)	Loss 6.4502e-01 (7.0052e-01) 
2023-05-24 23:50:50.741587: train Epoch: [1][14/24]	Time  1.600 ( 1.582)	Data  0.620 ( 0.454)	Loss 7.2432e-01 (7.0211e-01) 
2023-05-24 23:50:51.820477: train Epoch: [1][15/24]	Time  1.079 ( 1.550)	Data  0.001 ( 0.426)	Loss 6.7491e-01 (7.0041e-01) 
2023-05-24 23:50:53.665820: train Epoch: [1][16/24]	Time  1.845 ( 1.568)	Data  0.738 ( 0.444)	Loss 7.2995e-01 (7.0214e-01) 
2023-05-24 23:50:54.755015: train Epoch: [1][17/24]	Time  1.089 ( 1.541)	Data  0.001 ( 0.419)	Loss 6.6458e-01 (7.0006e-01) 
2023-05-24 23:50:56.590825: train Epoch: [1][18/24]	Time  1.836 ( 1.557)	Data  0.557 ( 0.427)	Loss 6.2175e-01 (6.9594e-01) 
2023-05-24 23:50:57.587422: train Epoch: [1][19/24]	Time  0.997 ( 1.529)	Data  0.001 ( 0.405)	Loss 7.0112e-01 (6.9620e-01) 
2023-05-24 23:50:59.182843: train Epoch: [1][20/24]	Time  1.595 ( 1.532)	Data  0.490 ( 0.409)	Loss 6.1516e-01 (6.9234e-01) 
2023-05-24 23:51:00.263812: train Epoch: [1][21/24]	Time  1.081 ( 1.511)	Data  0.001 ( 0.391)	Loss 7.0416e-01 (6.9287e-01) 
2023-05-24 23:51:02.043599: train Epoch: [1][22/24]	Time  1.780 ( 1.523)	Data  0.651 ( 0.402)	Loss 6.4722e-01 (6.9089e-01) 
2023-05-24 23:51:03.200047: train Epoch: [1][23/24]	Time  1.156 ( 1.508)	Data  0.001 ( 0.385)	Loss 5.9167e-01 (6.8676e-01) 
2023-05-24 23:51:03.255747: Train Epoch done in 36.23904272902291 s 
2023-05-24 23:51:06.679624: val Epoch: [1][0/9]	Time  2.353 ( 2.353)	Data  1.726 ( 1.726)	Loss 7.2299e-01 (7.2299e-01) 
2023-05-24 23:51:07.105061: val Epoch: [1][1/9]	Time  0.426 ( 1.389)	Data  0.002 ( 0.864)	Loss 5.7412e-01 (6.4855e-01) 
2023-05-24 23:51:07.834568: val Epoch: [1][2/9]	Time  0.730 ( 1.169)	Data  0.461 ( 0.729)	Loss 6.3345e-01 (6.4352e-01) 
2023-05-24 23:51:08.313194: val Epoch: [1][3/9]	Time  0.479 ( 0.997)	Data  0.001 ( 0.547)	Loss 6.2870e-01 (6.3981e-01) 
2023-05-24 23:51:09.322273: val Epoch: [1][4/9]	Time  1.009 ( 0.999)	Data  0.737 ( 0.585)	Loss 7.0763e-01 (6.5338e-01) 
2023-05-24 23:51:09.592660: val Epoch: [1][5/9]	Time  0.270 ( 0.878)	Data  0.001 ( 0.488)	Loss 7.3864e-01 (6.6759e-01) 
2023-05-24 23:51:10.902565: val Epoch: [1][6/9]	Time  1.310 ( 0.939)	Data  0.814 ( 0.534)	Loss 7.0947e-01 (6.7357e-01) 
2023-05-24 23:51:11.219438: val Epoch: [1][7/9]	Time  0.317 ( 0.862)	Data  0.001 ( 0.468)	Loss 6.3218e-01 (6.6840e-01) 
2023-05-24 23:51:12.077909: val Epoch: [1][8/9]	Time  0.858 ( 0.861)	Data  0.332 ( 0.453)	Loss 6.2359e-01 (6.6342e-01) 
2023-05-24 23:51:12.331164: Epoch 1 :Val : ['ET : 0.0', 'TC : 8.819184586172923e-05', 'WT : 0.00866026896983385'] 
2023-05-24 23:51:12.333682: Epoch 1 :Val : ['ET : 0.0', 'TC : 8.819184586172923e-05', 'WT : 0.00866026896983385'] 
2023-05-24 23:51:12.337837: Val epoch done in 9.082092144992203 s 
2023-05-24 23:51:12.344345: Batches per epoch:  24 
2023-05-24 23:51:16.469438: train Epoch: [2][ 0/24]	Time  4.124 ( 4.124)	Data  3.083 ( 3.083)	Loss 5.9451e-01 (5.9451e-01) 
2023-05-24 23:51:17.559589: train Epoch: [2][ 1/24]	Time  1.090 ( 2.607)	Data  0.001 ( 1.542)	Loss 6.1979e-01 (6.0715e-01) 
2023-05-24 23:51:19.328150: train Epoch: [2][ 2/24]	Time  1.769 ( 2.328)	Data  0.459 ( 1.181)	Loss 5.6941e-01 (5.9457e-01) 
2023-05-24 23:51:20.371370: train Epoch: [2][ 3/24]	Time  1.043 ( 2.007)	Data  0.001 ( 0.886)	Loss 6.0536e-01 (5.9727e-01) 
2023-05-24 23:51:22.032378: train Epoch: [2][ 4/24]	Time  1.661 ( 1.937)	Data  0.352 ( 0.779)	Loss 6.5986e-01 (6.0979e-01) 
2023-05-24 23:51:23.099901: train Epoch: [2][ 5/24]	Time  1.068 ( 1.792)	Data  0.001 ( 0.650)	Loss 6.1543e-01 (6.1073e-01) 
2023-05-24 23:51:24.405130: train Epoch: [2][ 6/24]	Time  1.305 ( 1.723)	Data  0.307 ( 0.601)	Loss 6.8510e-01 (6.2135e-01) 
2023-05-24 23:51:25.482938: train Epoch: [2][ 7/24]	Time  1.078 ( 1.642)	Data  0.001 ( 0.526)	Loss 6.2710e-01 (6.2207e-01) 
2023-05-24 23:51:27.419326: train Epoch: [2][ 8/24]	Time  1.936 ( 1.675)	Data  0.811 ( 0.557)	Loss 5.2717e-01 (6.1152e-01) 
2023-05-24 23:51:28.618368: train Epoch: [2][ 9/24]	Time  1.199 ( 1.627)	Data  0.001 ( 0.502)	Loss 6.0423e-01 (6.1080e-01) 
2023-05-24 23:51:30.343913: train Epoch: [2][10/24]	Time  1.726 ( 1.636)	Data  0.645 ( 0.515)	Loss 6.7092e-01 (6.1626e-01) 
2023-05-24 23:51:31.565580: train Epoch: [2][11/24]	Time  1.222 ( 1.602)	Data  0.001 ( 0.472)	Loss 5.6585e-01 (6.1206e-01) 
2023-05-24 23:51:33.161786: train Epoch: [2][12/24]	Time  1.596 ( 1.601)	Data  0.515 ( 0.475)	Loss 5.8634e-01 (6.1008e-01) 
2023-05-24 23:51:34.308185: train Epoch: [2][13/24]	Time  1.146 ( 1.569)	Data  0.001 ( 0.441)	Loss 6.5174e-01 (6.1306e-01) 
2023-05-24 23:51:35.840345: train Epoch: [2][14/24]	Time  1.532 ( 1.566)	Data  0.558 ( 0.449)	Loss 5.5301e-01 (6.0905e-01) 
2023-05-24 23:51:36.846757: train Epoch: [2][15/24]	Time  1.006 ( 1.531)	Data  0.001 ( 0.421)	Loss 5.9915e-01 (6.0844e-01) 
2023-05-24 23:51:38.575164: train Epoch: [2][16/24]	Time  1.728 ( 1.543)	Data  0.786 ( 0.443)	Loss 4.5243e-01 (5.9926e-01) 
2023-05-24 23:51:39.538579: train Epoch: [2][17/24]	Time  0.963 ( 1.511)	Data  0.001 ( 0.418)	Loss 6.1008e-01 (5.9986e-01) 
2023-05-24 23:51:41.653089: train Epoch: [2][18/24]	Time  2.115 ( 1.543)	Data  0.837 ( 0.440)	Loss 6.0146e-01 (5.9994e-01) 
2023-05-24 23:51:42.645505: train Epoch: [2][19/24]	Time  0.992 ( 1.515)	Data  0.001 ( 0.418)	Loss 4.1658e-01 (5.9078e-01) 
2023-05-24 23:51:44.392855: train Epoch: [2][20/24]	Time  1.747 ( 1.526)	Data  0.434 ( 0.419)	Loss 3.7208e-01 (5.8036e-01) 
2023-05-24 23:51:45.373886: train Epoch: [2][21/24]	Time  0.981 ( 1.501)	Data  0.001 ( 0.400)	Loss 4.0587e-01 (5.7243e-01) 
2023-05-24 23:51:47.009068: train Epoch: [2][22/24]	Time  1.635 ( 1.507)	Data  0.428 ( 0.401)	Loss 6.4408e-01 (5.7555e-01) 
2023-05-24 23:51:48.103432: train Epoch: [2][23/24]	Time  1.094 ( 1.490)	Data  0.001 ( 0.384)	Loss 3.7082e-01 (5.6702e-01) 
2023-05-24 23:51:48.156295: Train Epoch done in 35.812003590981476 s 
2023-05-24 23:51:51.289903: val Epoch: [2][0/9]	Time  2.164 ( 2.164)	Data  1.731 ( 1.731)	Loss 3.5418e-01 (3.5418e-01) 
2023-05-24 23:51:51.611240: val Epoch: [2][1/9]	Time  0.322 ( 1.243)	Data  0.002 ( 0.866)	Loss 3.7758e-01 (3.6588e-01) 
2023-05-24 23:51:52.878452: val Epoch: [2][2/9]	Time  1.267 ( 1.251)	Data  0.664 ( 0.799)	Loss 5.8530e-01 (4.3902e-01) 
2023-05-24 23:51:53.305712: val Epoch: [2][3/9]	Time  0.427 ( 1.045)	Data  0.001 ( 0.599)	Loss 4.3823e-01 (4.3882e-01) 
2023-05-24 23:51:53.950637: val Epoch: [2][4/9]	Time  0.645 ( 0.965)	Data  0.343 ( 0.548)	Loss 6.7674e-01 (4.8640e-01) 
2023-05-24 23:51:54.302210: val Epoch: [2][5/9]	Time  0.352 ( 0.863)	Data  0.086 ( 0.471)	Loss 5.7142e-01 (5.0057e-01) 
2023-05-24 23:51:55.579097: val Epoch: [2][6/9]	Time  1.277 ( 0.922)	Data  0.787 ( 0.516)	Loss 5.7990e-01 (5.1191e-01) 
2023-05-24 23:51:55.968533: val Epoch: [2][7/9]	Time  0.389 ( 0.855)	Data  0.001 ( 0.452)	Loss 4.0371e-01 (4.9838e-01) 
2023-05-24 23:51:56.610785: val Epoch: [2][8/9]	Time  0.642 ( 0.832)	Data  0.430 ( 0.449)	Loss 4.8528e-01 (4.9693e-01) 
2023-05-24 23:51:56.902096: Epoch 2 :Val : ['ET : 0.12961603701114655', 'TC : 0.32969793677330017', 'WT : 0.5620555877685547'] 
2023-05-24 23:51:56.905481: Epoch 2 :Val : ['ET : 0.12961603701114655', 'TC : 0.32969793677330017', 'WT : 0.5620555877685547'] 
2023-05-24 23:51:56.910334: Saving the model with DSC 0.3404565453529358 
2023-05-24 23:51:58.001379: Val epoch done in 9.845074353041127 s 
2023-05-24 23:51:58.008374: Batches per epoch:  24 
2023-05-24 23:52:02.633739: train Epoch: [3][ 0/24]	Time  4.625 ( 4.625)	Data  3.263 ( 3.263)	Loss 5.6244e-01 (5.6244e-01) 
2023-05-24 23:52:03.843092: train Epoch: [3][ 1/24]	Time  1.209 ( 2.917)	Data  0.001 ( 1.632)	Loss 5.9901e-01 (5.8072e-01) 
2023-05-24 23:52:05.115087: train Epoch: [3][ 2/24]	Time  1.272 ( 2.369)	Data  0.202 ( 1.155)	Loss 4.5170e-01 (5.3772e-01) 
2023-05-24 23:52:06.069077: train Epoch: [3][ 3/24]	Time  0.954 ( 2.015)	Data  0.001 ( 0.867)	Loss 5.1488e-01 (5.3201e-01) 
2023-05-24 23:52:07.807718: train Epoch: [3][ 4/24]	Time  1.739 ( 1.960)	Data  0.772 ( 0.848)	Loss 5.4003e-01 (5.3361e-01) 
2023-05-24 23:52:08.811410: train Epoch: [3][ 5/24]	Time  1.004 ( 1.800)	Data  0.001 ( 0.707)	Loss 4.6926e-01 (5.2289e-01) 
2023-05-24 23:52:10.872517: train Epoch: [3][ 6/24]	Time  2.061 ( 1.838)	Data  0.818 ( 0.723)	Loss 3.5851e-01 (4.9940e-01) 
2023-05-24 23:52:11.867417: train Epoch: [3][ 7/24]	Time  0.995 ( 1.732)	Data  0.001 ( 0.632)	Loss 4.8241e-01 (4.9728e-01) 
2023-05-24 23:52:13.597597: train Epoch: [3][ 8/24]	Time  1.730 ( 1.732)	Data  0.504 ( 0.618)	Loss 3.3486e-01 (4.7923e-01) 
2023-05-24 23:52:14.724292: train Epoch: [3][ 9/24]	Time  1.127 ( 1.672)	Data  0.001 ( 0.556)	Loss 4.6933e-01 (4.7824e-01) 
2023-05-24 23:52:16.117603: train Epoch: [3][10/24]	Time  1.393 ( 1.646)	Data  0.357 ( 0.538)	Loss 4.8667e-01 (4.7901e-01) 
2023-05-24 23:52:17.123845: train Epoch: [3][11/24]	Time  1.006 ( 1.593)	Data  0.001 ( 0.494)	Loss 3.3472e-01 (4.6699e-01) 
2023-05-24 23:52:18.796031: train Epoch: [3][12/24]	Time  1.672 ( 1.599)	Data  0.563 ( 0.499)	Loss 3.2408e-01 (4.5599e-01) 
2023-05-24 23:52:19.922847: train Epoch: [3][13/24]	Time  1.127 ( 1.565)	Data  0.001 ( 0.463)	Loss 2.7952e-01 (4.4339e-01) 
2023-05-24 23:52:21.643881: train Epoch: [3][14/24]	Time  1.721 ( 1.576)	Data  0.471 ( 0.464)	Loss 4.8923e-01 (4.4644e-01) 
2023-05-24 23:52:22.697124: train Epoch: [3][15/24]	Time  1.053 ( 1.543)	Data  0.001 ( 0.435)	Loss 3.6842e-01 (4.4157e-01) 
2023-05-24 23:52:24.286550: train Epoch: [3][16/24]	Time  1.589 ( 1.546)	Data  0.481 ( 0.438)	Loss 3.9071e-01 (4.3858e-01) 
2023-05-24 23:52:25.338681: train Epoch: [3][17/24]	Time  1.052 ( 1.518)	Data  0.001 ( 0.413)	Loss 2.8610e-01 (4.3010e-01) 
2023-05-24 23:52:27.282747: train Epoch: [3][18/24]	Time  1.944 ( 1.541)	Data  0.603 ( 0.423)	Loss 4.6284e-01 (4.3183e-01) 
2023-05-24 23:52:28.401217: train Epoch: [3][19/24]	Time  1.118 ( 1.520)	Data  0.001 ( 0.402)	Loss 5.2882e-01 (4.3668e-01) 
2023-05-24 23:52:29.846932: train Epoch: [3][20/24]	Time  1.446 ( 1.516)	Data  0.374 ( 0.401)	Loss 2.6503e-01 (4.2850e-01) 
2023-05-24 23:52:31.002562: train Epoch: [3][21/24]	Time  1.156 ( 1.500)	Data  0.001 ( 0.383)	Loss 4.4314e-01 (4.2917e-01) 
2023-05-24 23:52:32.780552: train Epoch: [3][22/24]	Time  1.778 ( 1.512)	Data  0.714 ( 0.397)	Loss 3.1284e-01 (4.2411e-01) 
2023-05-24 23:52:33.790954: train Epoch: [3][23/24]	Time  1.010 ( 1.491)	Data  0.001 ( 0.381)	Loss 2.6611e-01 (4.1753e-01) 
2023-05-24 23:52:33.846311: Train Epoch done in 35.83801297598984 s 
2023-05-24 23:52:36.907577: val Epoch: [3][0/9]	Time  2.103 ( 2.103)	Data  1.692 ( 1.692)	Loss 5.6363e-01 (5.6363e-01) 
2023-05-24 23:52:37.247581: val Epoch: [3][1/9]	Time  0.340 ( 1.222)	Data  0.002 ( 0.847)	Loss 5.3047e-01 (5.4705e-01) 
2023-05-24 23:52:38.110421: val Epoch: [3][2/9]	Time  0.863 ( 1.102)	Data  0.542 ( 0.745)	Loss 5.1036e-01 (5.3482e-01) 
2023-05-24 23:52:38.339361: val Epoch: [3][3/9]	Time  0.229 ( 0.884)	Data  0.001 ( 0.559)	Loss 3.3792e-01 (4.8560e-01) 
2023-05-24 23:52:39.607090: val Epoch: [3][4/9]	Time  1.268 ( 0.961)	Data  0.824 ( 0.612)	Loss 2.9818e-01 (4.4811e-01) 
2023-05-24 23:52:40.027882: val Epoch: [3][5/9]	Time  0.421 ( 0.871)	Data  0.001 ( 0.510)	Loss 3.7779e-01 (4.3639e-01) 
2023-05-24 23:52:40.773196: val Epoch: [3][6/9]	Time  0.745 ( 0.853)	Data  0.526 ( 0.513)	Loss 2.7674e-01 (4.1358e-01) 
2023-05-24 23:52:41.176291: val Epoch: [3][7/9]	Time  0.403 ( 0.797)	Data  0.104 ( 0.461)	Loss 4.1326e-01 (4.1354e-01) 
2023-05-24 23:52:42.093861: val Epoch: [3][8/9]	Time  0.918 ( 0.810)	Data  0.611 ( 0.478)	Loss 4.3971e-01 (4.1645e-01) 
2023-05-24 23:52:42.406252: Epoch 3 :Val : ['ET : 0.26242002844810486', 'TC : 0.49059242010116577', 'WT : 0.6413781642913818'] 
2023-05-24 23:52:42.409593: Epoch 3 :Val : ['ET : 0.26242002844810486', 'TC : 0.49059242010116577', 'WT : 0.6413781642913818'] 
2023-05-24 23:52:42.411794: Saving the model with DSC 0.46479690074920654 
2023-05-24 23:52:43.322212: Val epoch done in 9.475885603926145 s 
2023-05-24 23:52:43.351727: Batches per epoch:  24 
2023-05-24 23:52:47.845587: train Epoch: [4][ 0/24]	Time  4.493 ( 4.493)	Data  3.372 ( 3.372)	Loss 3.4241e-01 (3.4241e-01) 
2023-05-24 23:52:48.868109: train Epoch: [4][ 1/24]	Time  1.023 ( 2.758)	Data  0.001 ( 1.687)	Loss 4.3170e-01 (3.8706e-01) 
2023-05-24 23:52:50.735639: train Epoch: [4][ 2/24]	Time  1.868 ( 2.461)	Data  0.598 ( 1.324)	Loss 4.8115e-01 (4.1842e-01) 
2023-05-24 23:52:51.684427: train Epoch: [4][ 3/24]	Time  0.949 ( 2.083)	Data  0.001 ( 0.993)	Loss 4.4150e-01 (4.2419e-01) 
2023-05-24 23:52:53.377646: train Epoch: [4][ 4/24]	Time  1.693 ( 2.005)	Data  0.488 ( 0.892)	Loss 2.4866e-01 (3.8909e-01) 
2023-05-24 23:52:54.440438: train Epoch: [4][ 5/24]	Time  1.063 ( 1.848)	Data  0.001 ( 0.743)	Loss 2.3866e-01 (3.6402e-01) 
2023-05-24 23:52:55.969846: train Epoch: [4][ 6/24]	Time  1.529 ( 1.803)	Data  0.461 ( 0.703)	Loss 5.1871e-01 (3.8611e-01) 
2023-05-24 23:52:57.126084: train Epoch: [4][ 7/24]	Time  1.156 ( 1.722)	Data  0.001 ( 0.615)	Loss 4.7304e-01 (3.9698e-01) 
2023-05-24 23:52:58.938726: train Epoch: [4][ 8/24]	Time  1.813 ( 1.732)	Data  0.543 ( 0.607)	Loss 3.7533e-01 (3.9458e-01) 
2023-05-24 23:53:00.085918: train Epoch: [4][ 9/24]	Time  1.147 ( 1.673)	Data  0.001 ( 0.547)	Loss 3.4165e-01 (3.8928e-01) 
2023-05-24 23:53:01.357677: train Epoch: [4][10/24]	Time  1.272 ( 1.637)	Data  0.293 ( 0.524)	Loss 2.8348e-01 (3.7966e-01) 
2023-05-24 23:53:02.556893: train Epoch: [4][11/24]	Time  1.199 ( 1.600)	Data  0.001 ( 0.480)	Loss 3.9104e-01 (3.8061e-01) 
2023-05-24 23:53:04.046380: train Epoch: [4][12/24]	Time  1.489 ( 1.592)	Data  0.443 ( 0.477)	Loss 3.1249e-01 (3.7537e-01) 
2023-05-24 23:53:05.179587: train Epoch: [4][13/24]	Time  1.133 ( 1.559)	Data  0.001 ( 0.443)	Loss 2.2693e-01 (3.6477e-01) 
2023-05-24 23:53:06.908207: train Epoch: [4][14/24]	Time  1.729 ( 1.570)	Data  0.565 ( 0.451)	Loss 2.0578e-01 (3.5417e-01) 
2023-05-24 23:53:07.940737: train Epoch: [4][15/24]	Time  1.033 ( 1.537)	Data  0.001 ( 0.423)	Loss 2.8971e-01 (3.5014e-01) 
2023-05-24 23:53:09.815360: train Epoch: [4][16/24]	Time  1.875 ( 1.557)	Data  0.674 ( 0.438)	Loss 4.3647e-01 (3.5522e-01) 
2023-05-24 23:53:10.823673: train Epoch: [4][17/24]	Time  1.008 ( 1.526)	Data  0.001 ( 0.414)	Loss 3.2306e-01 (3.5343e-01) 
2023-05-24 23:53:12.553251: train Epoch: [4][18/24]	Time  1.730 ( 1.537)	Data  0.484 ( 0.417)	Loss 3.6331e-01 (3.5395e-01) 
2023-05-24 23:53:13.585928: train Epoch: [4][19/24]	Time  1.033 ( 1.512)	Data  0.001 ( 0.397)	Loss 3.0819e-01 (3.5166e-01) 
2023-05-24 23:53:15.036073: train Epoch: [4][20/24]	Time  1.450 ( 1.509)	Data  0.449 ( 0.399)	Loss 3.5362e-01 (3.5176e-01) 
2023-05-24 23:53:16.031200: train Epoch: [4][21/24]	Time  0.995 ( 1.485)	Data  0.001 ( 0.381)	Loss 1.7339e-01 (3.4365e-01) 
2023-05-24 23:53:18.157805: train Epoch: [4][22/24]	Time  2.127 ( 1.513)	Data  0.984 ( 0.407)	Loss 4.1583e-01 (3.4679e-01) 
2023-05-24 23:53:19.137572: train Epoch: [4][23/24]	Time  0.980 ( 1.491)	Data  0.001 ( 0.390)	Loss 2.7701e-01 (3.4388e-01) 
2023-05-24 23:53:19.276531: Train Epoch done in 35.92487492901273 s 
2023-05-24 23:53:22.318837: val Epoch: [4][0/9]	Time  2.032 ( 2.032)	Data  1.537 ( 1.537)	Loss 5.6617e-01 (5.6617e-01) 
2023-05-24 23:53:22.703507: val Epoch: [4][1/9]	Time  0.385 ( 1.209)	Data  0.003 ( 0.770)	Loss 2.0074e-01 (3.8346e-01) 
2023-05-24 23:53:23.834470: val Epoch: [4][2/9]	Time  1.131 ( 1.183)	Data  0.635 ( 0.725)	Loss 4.6661e-01 (4.1117e-01) 
2023-05-24 23:53:24.309412: val Epoch: [4][3/9]	Time  0.475 ( 1.006)	Data  0.001 ( 0.544)	Loss 4.0066e-01 (4.0855e-01) 
2023-05-24 23:53:25.367805: val Epoch: [4][4/9]	Time  1.058 ( 1.016)	Data  0.678 ( 0.571)	Loss 4.3477e-01 (4.1379e-01) 
2023-05-24 23:53:25.678162: val Epoch: [4][5/9]	Time  0.310 ( 0.899)	Data  0.001 ( 0.476)	Loss 2.6566e-01 (3.8910e-01) 
2023-05-24 23:53:26.689851: val Epoch: [4][6/9]	Time  1.012 ( 0.915)	Data  0.750 ( 0.515)	Loss 2.5744e-01 (3.7029e-01) 
2023-05-24 23:53:27.175699: val Epoch: [4][7/9]	Time  0.486 ( 0.861)	Data  0.001 ( 0.451)	Loss 3.7930e-01 (3.7142e-01) 
2023-05-24 23:53:27.780776: val Epoch: [4][8/9]	Time  0.605 ( 0.833)	Data  0.374 ( 0.442)	Loss 2.5747e-01 (3.5876e-01) 
2023-05-24 23:53:28.013032: Epoch 4 :Val : ['ET : 0.31978172063827515', 'TC : 0.535900354385376', 'WT : 0.6744715571403503'] 
2023-05-24 23:53:28.014021: Epoch 4 :Val : ['ET : 0.31978172063827515', 'TC : 0.535900354385376', 'WT : 0.6744715571403503'] 
2023-05-24 23:53:28.018309: Saving the model with DSC 0.5100512504577637 
2023-05-24 23:53:28.944470: Val epoch done in 9.667921717977151 s 
2023-05-24 23:53:28.951003: Batches per epoch:  24 
2023-05-24 23:53:33.421072: train Epoch: [5][ 0/24]	Time  4.470 ( 4.470)	Data  3.198 ( 3.198)	Loss 1.8484e-01 (1.8484e-01) 
2023-05-24 23:53:34.412686: train Epoch: [5][ 1/24]	Time  0.992 ( 2.731)	Data  0.002 ( 1.600)	Loss 2.5696e-01 (2.2090e-01) 
2023-05-24 23:53:36.202204: train Epoch: [5][ 2/24]	Time  1.789 ( 2.417)	Data  0.639 ( 1.280)	Loss 3.1526e-01 (2.5235e-01) 
2023-05-24 23:53:37.229570: train Epoch: [5][ 3/24]	Time  1.027 ( 2.070)	Data  0.001 ( 0.960)	Loss 4.5732e-01 (3.0360e-01) 
2023-05-24 23:53:39.195462: train Epoch: [5][ 4/24]	Time  1.966 ( 2.049)	Data  0.682 ( 0.904)	Loss 3.6287e-01 (3.1545e-01) 
2023-05-24 23:53:40.188054: train Epoch: [5][ 5/24]	Time  0.993 ( 1.873)	Data  0.001 ( 0.754)	Loss 3.3583e-01 (3.1885e-01) 
2023-05-24 23:53:42.242629: train Epoch: [5][ 6/24]	Time  2.055 ( 1.899)	Data  0.638 ( 0.737)	Loss 1.9813e-01 (3.0160e-01) 
2023-05-24 23:53:43.328061: train Epoch: [5][ 7/24]	Time  1.085 ( 1.797)	Data  0.001 ( 0.645)	Loss 2.5675e-01 (2.9600e-01) 
2023-05-24 23:53:44.577920: train Epoch: [5][ 8/24]	Time  1.250 ( 1.736)	Data  0.218 ( 0.598)	Loss 4.2904e-01 (3.1078e-01) 
2023-05-24 23:53:45.701062: train Epoch: [5][ 9/24]	Time  1.123 ( 1.675)	Data  0.001 ( 0.538)	Loss 3.1047e-01 (3.1075e-01) 
2023-05-24 23:53:47.493630: train Epoch: [5][10/24]	Time  1.793 ( 1.686)	Data  0.674 ( 0.550)	Loss 2.4188e-01 (3.0449e-01) 
2023-05-24 23:53:48.580495: train Epoch: [5][11/24]	Time  1.087 ( 1.636)	Data  0.001 ( 0.505)	Loss 2.5537e-01 (3.0040e-01) 
2023-05-24 23:53:50.323602: train Epoch: [5][12/24]	Time  1.743 ( 1.644)	Data  0.608 ( 0.513)	Loss 2.9669e-01 (3.0011e-01) 
2023-05-24 23:53:51.393598: train Epoch: [5][13/24]	Time  1.070 ( 1.603)	Data  0.001 ( 0.476)	Loss 3.5739e-01 (3.0420e-01) 
2023-05-24 23:53:53.146096: train Epoch: [5][14/24]	Time  1.752 ( 1.613)	Data  0.671 ( 0.489)	Loss 4.3432e-01 (3.1288e-01) 
2023-05-24 23:53:54.313871: train Epoch: [5][15/24]	Time  1.168 ( 1.585)	Data  0.001 ( 0.459)	Loss 3.9840e-01 (3.1822e-01) 
2023-05-24 23:53:56.033144: train Epoch: [5][16/24]	Time  1.719 ( 1.593)	Data  0.629 ( 0.469)	Loss 3.0481e-01 (3.1743e-01) 
2023-05-24 23:53:57.079154: train Epoch: [5][17/24]	Time  1.046 ( 1.563)	Data  0.001 ( 0.443)	Loss 2.4697e-01 (3.1352e-01) 
2023-05-24 23:53:58.835764: train Epoch: [5][18/24]	Time  1.757 ( 1.573)	Data  0.773 ( 0.460)	Loss 1.9589e-01 (3.0733e-01) 
2023-05-24 23:53:59.958434: train Epoch: [5][19/24]	Time  1.123 ( 1.550)	Data  0.001 ( 0.437)	Loss 3.7953e-01 (3.1094e-01) 
2023-05-24 23:54:02.055321: train Epoch: [5][20/24]	Time  2.097 ( 1.576)	Data  0.839 ( 0.456)	Loss 3.3577e-01 (3.1212e-01) 
2023-05-24 23:54:03.102833: train Epoch: [5][21/24]	Time  1.048 ( 1.552)	Data  0.001 ( 0.436)	Loss 3.1169e-01 (3.1210e-01) 
2023-05-24 23:54:04.575351: train Epoch: [5][22/24]	Time  1.472 ( 1.549)	Data  0.423 ( 0.435)	Loss 4.3429e-01 (3.1741e-01) 
2023-05-24 23:54:05.691977: train Epoch: [5][23/24]	Time  1.117 ( 1.531)	Data  0.001 ( 0.417)	Loss 1.9634e-01 (3.1237e-01) 
2023-05-24 23:54:05.749135: Train Epoch done in 36.79818839998916 s 
2023-05-24 23:54:09.129297: val Epoch: [5][0/9]	Time  2.291 ( 2.291)	Data  1.781 ( 1.781)	Loss 2.4688e-01 (2.4688e-01) 
2023-05-24 23:54:09.500159: val Epoch: [5][1/9]	Time  0.371 ( 1.331)	Data  0.012 ( 0.897)	Loss 3.9075e-01 (3.1881e-01) 
2023-05-24 23:54:10.470274: val Epoch: [5][2/9]	Time  0.970 ( 1.211)	Data  0.516 ( 0.770)	Loss 4.7424e-01 (3.7062e-01) 
2023-05-24 23:54:11.049916: val Epoch: [5][3/9]	Time  0.580 ( 1.053)	Data  0.001 ( 0.578)	Loss 3.3497e-01 (3.6171e-01) 
2023-05-24 23:54:11.762615: val Epoch: [5][4/9]	Time  0.713 ( 0.985)	Data  0.438 ( 0.550)	Loss 2.3150e-01 (3.3567e-01) 
2023-05-24 23:54:12.266114: val Epoch: [5][5/9]	Time  0.504 ( 0.905)	Data  0.001 ( 0.458)	Loss 3.4671e-01 (3.3751e-01) 
2023-05-24 23:54:13.367964: val Epoch: [5][6/9]	Time  1.102 ( 0.933)	Data  0.599 ( 0.478)	Loss 2.6749e-01 (3.2751e-01) 
2023-05-24 23:54:13.635509: val Epoch: [5][7/9]	Time  0.268 ( 0.850)	Data  0.001 ( 0.419)	Loss 2.3965e-01 (3.1652e-01) 
2023-05-24 23:54:14.357350: val Epoch: [5][8/9]	Time  0.722 ( 0.836)	Data  0.465 ( 0.424)	Loss 4.0893e-01 (3.2679e-01) 
2023-05-24 23:54:14.610177: Epoch 5 :Val : ['ET : 0.38445210456848145', 'TC : 0.5717788338661194', 'WT : 0.6920635104179382'] 
2023-05-24 23:54:14.613180: Epoch 5 :Val : ['ET : 0.38445210456848145', 'TC : 0.5717788338661194', 'WT : 0.6920635104179382'] 
2023-05-24 23:54:14.615551: Saving the model with DSC 0.5494314432144165 
2023-05-24 23:54:15.529421: Val epoch done in 9.780285255983472 s 
2023-05-24 23:54:15.537530: Batches per epoch:  24 
2023-05-24 23:54:20.039676: train Epoch: [6][ 0/24]	Time  4.498 ( 4.498)	Data  3.332 ( 3.332)	Loss 4.2650e-01 (4.2650e-01) 
2023-05-24 23:54:21.093777: train Epoch: [6][ 1/24]	Time  1.054 ( 2.776)	Data  0.002 ( 1.667)	Loss 3.0151e-01 (3.6400e-01) 
2023-05-24 23:54:22.936627: train Epoch: [6][ 2/24]	Time  1.843 ( 2.465)	Data  0.573 ( 1.302)	Loss 4.9198e-01 (4.0666e-01) 
2023-05-24 23:54:24.043131: train Epoch: [6][ 3/24]	Time  1.107 ( 2.125)	Data  0.001 ( 0.977)	Loss 2.7463e-01 (3.7365e-01) 
2023-05-24 23:54:25.575086: train Epoch: [6][ 4/24]	Time  1.532 ( 2.007)	Data  0.369 ( 0.855)	Loss 2.5176e-01 (3.4927e-01) 
2023-05-24 23:54:26.585660: train Epoch: [6][ 5/24]	Time  1.011 ( 1.841)	Data  0.002 ( 0.713)	Loss 3.5258e-01 (3.4982e-01) 
2023-05-24 23:54:28.181002: train Epoch: [6][ 6/24]	Time  1.595 ( 1.806)	Data  0.511 ( 0.684)	Loss 2.7053e-01 (3.3850e-01) 
2023-05-24 23:54:29.351691: train Epoch: [6][ 7/24]	Time  1.171 ( 1.726)	Data  0.001 ( 0.599)	Loss 3.8358e-01 (3.4413e-01) 
2023-05-24 23:54:30.984349: train Epoch: [6][ 8/24]	Time  1.633 ( 1.716)	Data  0.557 ( 0.594)	Loss 2.6530e-01 (3.3537e-01) 
2023-05-24 23:54:32.215278: train Epoch: [6][ 9/24]	Time  1.231 ( 1.667)	Data  0.001 ( 0.535)	Loss 3.0275e-01 (3.3211e-01) 
2023-05-24 23:54:33.537200: train Epoch: [6][10/24]	Time  1.322 ( 1.636)	Data  0.321 ( 0.515)	Loss 2.2577e-01 (3.2244e-01) 
2023-05-24 23:54:34.603109: train Epoch: [6][11/24]	Time  1.066 ( 1.588)	Data  0.001 ( 0.473)	Loss 2.0606e-01 (3.1274e-01) 
2023-05-24 23:54:36.195776: train Epoch: [6][12/24]	Time  1.593 ( 1.589)	Data  0.643 ( 0.486)	Loss 1.9917e-01 (3.0401e-01) 
2023-05-24 23:54:37.183224: train Epoch: [6][13/24]	Time  0.987 ( 1.546)	Data  0.001 ( 0.451)	Loss 2.9387e-01 (3.0328e-01) 
2023-05-24 23:54:38.950756: train Epoch: [6][14/24]	Time  1.768 ( 1.561)	Data  0.774 ( 0.473)	Loss 2.2672e-01 (2.9818e-01) 
2023-05-24 23:54:39.973539: train Epoch: [6][15/24]	Time  1.023 ( 1.527)	Data  0.017 ( 0.444)	Loss 2.4171e-01 (2.9465e-01) 
2023-05-24 23:54:41.681998: train Epoch: [6][16/24]	Time  1.708 ( 1.538)	Data  0.658 ( 0.457)	Loss 3.6016e-01 (2.9850e-01) 
2023-05-24 23:54:42.968238: train Epoch: [6][17/24]	Time  1.286 ( 1.524)	Data  0.215 ( 0.443)	Loss 3.2158e-01 (2.9979e-01) 
2023-05-24 23:54:44.713552: train Epoch: [6][18/24]	Time  1.745 ( 1.535)	Data  0.479 ( 0.445)	Loss 2.8407e-01 (2.9896e-01) 
2023-05-24 23:54:45.795571: train Epoch: [6][19/24]	Time  1.082 ( 1.513)	Data  0.001 ( 0.423)	Loss 2.2197e-01 (2.9511e-01) 
2023-05-24 23:54:47.518455: train Epoch: [6][20/24]	Time  1.723 ( 1.523)	Data  0.633 ( 0.433)	Loss 3.0398e-01 (2.9553e-01) 
2023-05-24 23:54:48.981610: train Epoch: [6][21/24]	Time  1.463 ( 1.520)	Data  0.084 ( 0.417)	Loss 2.0505e-01 (2.9142e-01) 
2023-05-24 23:54:50.114927: train Epoch: [6][22/24]	Time  1.133 ( 1.503)	Data  0.170 ( 0.406)	Loss 2.6727e-01 (2.9037e-01) 
2023-05-24 23:54:51.178721: train Epoch: [6][23/24]	Time  1.064 ( 1.485)	Data  0.001 ( 0.389)	Loss 1.7803e-01 (2.8569e-01) 
2023-05-24 23:54:51.233274: Train Epoch done in 35.695808690041304 s 
2023-05-24 23:54:54.197243: val Epoch: [6][0/9]	Time  1.981 ( 1.981)	Data  1.729 ( 1.729)	Loss 3.8305e-01 (3.8305e-01) 
2023-05-24 23:54:54.474782: val Epoch: [6][1/9]	Time  0.278 ( 1.130)	Data  0.002 ( 0.865)	Loss 2.6426e-01 (3.2365e-01) 
2023-05-24 23:54:55.764650: val Epoch: [6][2/9]	Time  1.290 ( 1.183)	Data  0.781 ( 0.837)	Loss 2.4854e-01 (2.9862e-01) 
2023-05-24 23:54:56.170162: val Epoch: [6][3/9]	Time  0.406 ( 0.989)	Data  0.011 ( 0.631)	Loss 2.2858e-01 (2.8111e-01) 
2023-05-24 23:54:57.083956: val Epoch: [6][4/9]	Time  0.914 ( 0.974)	Data  0.515 ( 0.607)	Loss 4.7638e-01 (3.2016e-01) 
2023-05-24 23:54:57.414993: val Epoch: [6][5/9]	Time  0.331 ( 0.867)	Data  0.001 ( 0.506)	Loss 2.8267e-01 (3.1392e-01) 
2023-05-24 23:54:58.365534: val Epoch: [6][6/9]	Time  0.951 ( 0.879)	Data  0.738 ( 0.539)	Loss 4.5537e-01 (3.3412e-01) 
2023-05-24 23:54:58.803308: val Epoch: [6][7/9]	Time  0.438 ( 0.823)	Data  0.170 ( 0.493)	Loss 1.8116e-01 (3.1500e-01) 
2023-05-24 23:54:59.688178: val Epoch: [6][8/9]	Time  0.885 ( 0.830)	Data  0.542 ( 0.499)	Loss 3.6428e-01 (3.2048e-01) 
2023-05-24 23:54:59.953593: Epoch 6 :Val : ['ET : 0.3578341603279114', 'TC : 0.5802099108695984', 'WT : 0.6904938817024231'] 
2023-05-24 23:54:59.958731: Epoch 6 :Val : ['ET : 0.3578341603279114', 'TC : 0.5802099108695984', 'WT : 0.6904938817024231'] 
2023-05-24 23:54:59.961798: Val epoch done in 8.72852741589304 s 
2023-05-24 23:54:59.968386: Batches per epoch:  24 
2023-05-24 23:55:04.421711: train Epoch: [7][ 0/24]	Time  4.453 ( 4.453)	Data  3.224 ( 3.224)	Loss 2.5810e-01 (2.5810e-01) 
2023-05-24 23:55:05.591774: train Epoch: [7][ 1/24]	Time  1.170 ( 2.811)	Data  0.001 ( 1.613)	Loss 2.1672e-01 (2.3741e-01) 
2023-05-24 23:55:07.101215: train Epoch: [7][ 2/24]	Time  1.509 ( 2.377)	Data  0.476 ( 1.234)	Loss 3.6280e-01 (2.7921e-01) 
2023-05-24 23:55:08.275910: train Epoch: [7][ 3/24]	Time  1.175 ( 2.077)	Data  0.001 ( 0.926)	Loss 1.5083e-01 (2.4711e-01) 
2023-05-24 23:55:09.856152: train Epoch: [7][ 4/24]	Time  1.580 ( 1.977)	Data  0.546 ( 0.850)	Loss 2.3224e-01 (2.4414e-01) 
2023-05-24 23:55:10.846813: train Epoch: [7][ 5/24]	Time  0.991 ( 1.813)	Data  0.001 ( 0.708)	Loss 2.0941e-01 (2.3835e-01) 
2023-05-24 23:55:12.824989: train Epoch: [7][ 6/24]	Time  1.978 ( 1.837)	Data  0.761 ( 0.716)	Loss 3.3325e-01 (2.5191e-01) 
2023-05-24 23:55:13.804204: train Epoch: [7][ 7/24]	Time  0.979 ( 1.729)	Data  0.001 ( 0.626)	Loss 3.8787e-01 (2.6890e-01) 
2023-05-24 23:55:15.355432: train Epoch: [7][ 8/24]	Time  1.551 ( 1.710)	Data  0.523 ( 0.615)	Loss 3.5351e-01 (2.7830e-01) 
2023-05-24 23:55:16.482419: train Epoch: [7][ 9/24]	Time  1.127 ( 1.651)	Data  0.001 ( 0.553)	Loss 2.3802e-01 (2.7428e-01) 
2023-05-24 23:55:18.204983: train Epoch: [7][10/24]	Time  1.723 ( 1.658)	Data  0.617 ( 0.559)	Loss 1.8132e-01 (2.6583e-01) 
2023-05-24 23:55:19.513012: train Epoch: [7][11/24]	Time  1.308 ( 1.629)	Data  0.001 ( 0.513)	Loss 2.7037e-01 (2.6620e-01) 
2023-05-24 23:55:21.008529: train Epoch: [7][12/24]	Time  1.495 ( 1.618)	Data  0.357 ( 0.501)	Loss 3.0868e-01 (2.6947e-01) 
2023-05-24 23:55:22.329918: train Epoch: [7][13/24]	Time  1.321 ( 1.597)	Data  0.001 ( 0.465)	Loss 3.2200e-01 (2.7322e-01) 
2023-05-24 23:55:23.815324: train Epoch: [7][14/24]	Time  1.485 ( 1.590)	Data  0.280 ( 0.453)	Loss 3.1856e-01 (2.7625e-01) 
2023-05-24 23:55:24.871632: train Epoch: [7][15/24]	Time  1.056 ( 1.556)	Data  0.001 ( 0.424)	Loss 2.4857e-01 (2.7452e-01) 
2023-05-24 23:55:26.575100: train Epoch: [7][16/24]	Time  1.703 ( 1.565)	Data  0.483 ( 0.428)	Loss 4.2507e-01 (2.8337e-01) 
2023-05-24 23:55:27.567084: train Epoch: [7][17/24]	Time  0.992 ( 1.533)	Data  0.001 ( 0.404)	Loss 2.3058e-01 (2.8044e-01) 
2023-05-24 23:55:29.480901: train Epoch: [7][18/24]	Time  1.914 ( 1.553)	Data  0.648 ( 0.417)	Loss 3.7510e-01 (2.8542e-01) 
2023-05-24 23:55:30.462045: train Epoch: [7][19/24]	Time  0.981 ( 1.525)	Data  0.001 ( 0.396)	Loss 2.6263e-01 (2.8428e-01) 
2023-05-24 23:55:32.434072: train Epoch: [7][20/24]	Time  1.972 ( 1.546)	Data  0.622 ( 0.407)	Loss 1.6267e-01 (2.7849e-01) 
2023-05-24 23:55:33.414056: train Epoch: [7][21/24]	Time  0.980 ( 1.520)	Data  0.001 ( 0.389)	Loss 3.3794e-01 (2.8119e-01) 
2023-05-24 23:55:34.948579: train Epoch: [7][22/24]	Time  1.535 ( 1.521)	Data  0.515 ( 0.394)	Loss 2.2429e-01 (2.7872e-01) 
2023-05-24 23:55:35.992993: train Epoch: [7][23/24]	Time  1.044 ( 1.501)	Data  0.001 ( 0.378)	Loss 1.9874e-01 (2.7539e-01) 
2023-05-24 23:55:36.115115: Train Epoch done in 36.14679067302495 s 
2023-05-24 23:55:39.714074: val Epoch: [7][0/9]	Time  2.454 ( 2.454)	Data  1.954 ( 1.954)	Loss 4.1424e-01 (4.1424e-01) 
2023-05-24 23:55:40.045887: val Epoch: [7][1/9]	Time  0.332 ( 1.393)	Data  0.002 ( 0.978)	Loss 6.7893e-01 (5.4659e-01) 
2023-05-24 23:55:41.236072: val Epoch: [7][2/9]	Time  1.190 ( 1.325)	Data  0.667 ( 0.874)	Loss 2.8809e-01 (4.6042e-01) 
2023-05-24 23:55:41.787039: val Epoch: [7][3/9]	Time  0.551 ( 1.132)	Data  0.001 ( 0.656)	Loss 2.3404e-01 (4.0383e-01) 
2023-05-24 23:55:42.547805: val Epoch: [7][4/9]	Time  0.761 ( 1.058)	Data  0.430 ( 0.611)	Loss 2.6431e-01 (3.7592e-01) 
2023-05-24 23:55:42.796471: val Epoch: [7][5/9]	Time  0.249 ( 0.923)	Data  0.001 ( 0.509)	Loss 2.9535e-01 (3.6249e-01) 
2023-05-24 23:55:43.886307: val Epoch: [7][6/9]	Time  1.090 ( 0.947)	Data  0.794 ( 0.550)	Loss 3.6813e-01 (3.6330e-01) 
2023-05-24 23:55:44.237077: val Epoch: [7][7/9]	Time  0.351 ( 0.872)	Data  0.001 ( 0.481)	Loss 2.0214e-01 (3.4316e-01) 
2023-05-24 23:55:45.100675: val Epoch: [7][8/9]	Time  0.864 ( 0.871)	Data  0.541 ( 0.488)	Loss 4.3278e-01 (3.5311e-01) 
2023-05-24 23:55:45.393013: Epoch 7 :Val : ['ET : 0.31239551305770874', 'TC : 0.532098650932312', 'WT : 0.6321912407875061'] 
2023-05-24 23:55:45.395608: Epoch 7 :Val : ['ET : 0.31239551305770874', 'TC : 0.532098650932312', 'WT : 0.6321912407875061'] 
2023-05-24 23:55:45.399595: Val epoch done in 9.284481214010157 s 
2023-05-24 23:55:45.407210: Batches per epoch:  24 
2023-05-24 23:55:50.426805: train Epoch: [8][ 0/24]	Time  5.019 ( 5.019)	Data  3.598 ( 3.598)	Loss 2.8891e-01 (2.8891e-01) 
2023-05-24 23:55:51.593379: train Epoch: [8][ 1/24]	Time  1.167 ( 3.093)	Data  0.002 ( 1.800)	Loss 1.6932e-01 (2.2912e-01) 
2023-05-24 23:55:53.029098: train Epoch: [8][ 2/24]	Time  1.436 ( 2.540)	Data  0.328 ( 1.309)	Loss 2.9663e-01 (2.5162e-01) 
2023-05-24 23:55:54.155815: train Epoch: [8][ 3/24]	Time  1.127 ( 2.187)	Data  0.001 ( 0.982)	Loss 2.4522e-01 (2.5002e-01) 
2023-05-24 23:55:55.997142: train Epoch: [8][ 4/24]	Time  1.841 ( 2.118)	Data  0.688 ( 0.923)	Loss 2.8606e-01 (2.5723e-01) 
2023-05-24 23:55:56.975782: train Epoch: [8][ 5/24]	Time  0.979 ( 1.928)	Data  0.001 ( 0.770)	Loss 1.9144e-01 (2.4626e-01) 
2023-05-24 23:55:59.170537: train Epoch: [8][ 6/24]	Time  2.195 ( 1.966)	Data  0.996 ( 0.802)	Loss 4.3095e-01 (2.7265e-01) 
2023-05-24 23:56:00.322260: train Epoch: [8][ 7/24]	Time  1.152 ( 1.864)	Data  0.001 ( 0.702)	Loss 3.5298e-01 (2.8269e-01) 
2023-05-24 23:56:02.119178: train Epoch: [8][ 8/24]	Time  1.797 ( 1.857)	Data  0.516 ( 0.681)	Loss 2.5995e-01 (2.8016e-01) 
2023-05-24 23:56:03.265568: train Epoch: [8][ 9/24]	Time  1.146 ( 1.786)	Data  0.001 ( 0.613)	Loss 1.8433e-01 (2.7058e-01) 
2023-05-24 23:56:04.850545: train Epoch: [8][10/24]	Time  1.585 ( 1.768)	Data  0.415 ( 0.595)	Loss 2.0160e-01 (2.6431e-01) 
2023-05-24 23:56:06.327247: train Epoch: [8][11/24]	Time  1.477 ( 1.743)	Data  0.001 ( 0.546)	Loss 2.5449e-01 (2.6349e-01) 
2023-05-24 23:56:07.697828: train Epoch: [8][12/24]	Time  1.371 ( 1.715)	Data  0.179 ( 0.517)	Loss 2.3567e-01 (2.6135e-01) 
2023-05-24 23:56:08.749562: train Epoch: [8][13/24]	Time  1.052 ( 1.667)	Data  0.001 ( 0.481)	Loss 2.4074e-01 (2.5988e-01) 
2023-05-24 23:56:10.479929: train Epoch: [8][14/24]	Time  1.730 ( 1.671)	Data  0.732 ( 0.497)	Loss 5.2573e-01 (2.7760e-01) 
2023-05-24 23:56:11.565427: train Epoch: [8][15/24]	Time  1.086 ( 1.635)	Data  0.001 ( 0.466)	Loss 1.4457e-01 (2.6929e-01) 
2023-05-24 23:56:13.483885: train Epoch: [8][16/24]	Time  1.918 ( 1.652)	Data  0.858 ( 0.489)	Loss 1.7737e-01 (2.6388e-01) 
2023-05-24 23:56:14.565210: train Epoch: [8][17/24]	Time  1.081 ( 1.620)	Data  0.001 ( 0.462)	Loss 2.0532e-01 (2.6063e-01) 
2023-05-24 23:56:16.368888: train Epoch: [8][18/24]	Time  1.804 ( 1.630)	Data  0.731 ( 0.476)	Loss 1.9543e-01 (2.5720e-01) 
2023-05-24 23:56:17.479802: train Epoch: [8][19/24]	Time  1.111 ( 1.604)	Data  0.001 ( 0.453)	Loss 2.2637e-01 (2.5565e-01) 
2023-05-24 23:56:19.399808: train Epoch: [8][20/24]	Time  1.920 ( 1.619)	Data  0.828 ( 0.470)	Loss 2.2487e-01 (2.5419e-01) 
2023-05-24 23:56:20.409643: train Epoch: [8][21/24]	Time  1.010 ( 1.591)	Data  0.001 ( 0.449)	Loss 3.8880e-01 (2.6031e-01) 
2023-05-24 23:56:21.688853: train Epoch: [8][22/24]	Time  1.279 ( 1.577)	Data  0.330 ( 0.444)	Loss 2.7930e-01 (2.6113e-01) 
2023-05-24 23:56:22.745071: train Epoch: [8][23/24]	Time  1.056 ( 1.556)	Data  0.001 ( 0.426)	Loss 4.1806e-01 (2.6767e-01) 
2023-05-24 23:56:22.802764: Train Epoch done in 37.39561407396104 s 
2023-05-24 23:56:25.623182: val Epoch: [8][0/9]	Time  1.901 ( 1.901)	Data  1.644 ( 1.644)	Loss 3.2134e-01 (3.2134e-01) 
2023-05-24 23:56:26.106539: val Epoch: [8][1/9]	Time  0.484 ( 1.192)	Data  0.002 ( 0.823)	Loss 2.6648e-01 (2.9391e-01) 
2023-05-24 23:56:27.231557: val Epoch: [8][2/9]	Time  1.125 ( 1.170)	Data  0.662 ( 0.769)	Loss 3.5302e-01 (3.1361e-01) 
2023-05-24 23:56:27.440679: val Epoch: [8][3/9]	Time  0.209 ( 0.930)	Data  0.001 ( 0.577)	Loss 2.6416e-01 (3.0125e-01) 
2023-05-24 23:56:28.531302: val Epoch: [8][4/9]	Time  1.091 ( 0.962)	Data  0.782 ( 0.618)	Loss 4.3659e-01 (3.2832e-01) 
2023-05-24 23:56:28.975565: val Epoch: [8][5/9]	Time  0.444 ( 0.876)	Data  0.001 ( 0.515)	Loss 3.7502e-01 (3.3610e-01) 
2023-05-24 23:56:29.858583: val Epoch: [8][6/9]	Time  0.883 ( 0.877)	Data  0.603 ( 0.528)	Loss 2.0784e-01 (3.1778e-01) 
2023-05-24 23:56:30.126090: val Epoch: [8][7/9]	Time  0.268 ( 0.800)	Data  0.006 ( 0.463)	Loss 2.2868e-01 (3.0664e-01) 
2023-05-24 23:56:31.110434: val Epoch: [8][8/9]	Time  0.984 ( 0.821)	Data  0.676 ( 0.486)	Loss 2.1733e-01 (2.9672e-01) 
2023-05-24 23:56:31.409546: Epoch 8 :Val : ['ET : 0.4209645688533783', 'TC : 0.6196916103363037', 'WT : 0.7237276434898376'] 
2023-05-24 23:56:31.412100: Epoch 8 :Val : ['ET : 0.4209645688533783', 'TC : 0.6196916103363037', 'WT : 0.7237276434898376'] 
2023-05-24 23:56:31.414103: Saving the model with DSC 0.5881279706954956 
2023-05-24 23:56:32.753846: Val epoch done in 9.951078183017671 s 
2023-05-24 23:56:32.759892: Batches per epoch:  24 
2023-05-24 23:56:36.992888: train Epoch: [9][ 0/24]	Time  4.232 ( 4.232)	Data  3.089 ( 3.089)	Loss 2.8565e-01 (2.8565e-01) 
2023-05-24 23:56:38.014025: train Epoch: [9][ 1/24]	Time  1.021 ( 2.627)	Data  0.001 ( 1.545)	Loss 1.4314e-01 (2.1439e-01) 
2023-05-24 23:56:40.052088: train Epoch: [9][ 2/24]	Time  2.038 ( 2.431)	Data  0.692 ( 1.261)	Loss 2.7257e-01 (2.3379e-01) 
2023-05-24 23:56:41.130101: train Epoch: [9][ 3/24]	Time  1.078 ( 2.092)	Data  0.001 ( 0.946)	Loss 3.2877e-01 (2.5753e-01) 
2023-05-24 23:56:42.695293: train Epoch: [9][ 4/24]	Time  1.565 ( 1.987)	Data  0.366 ( 0.830)	Loss 6.4881e-01 (3.3579e-01) 
2023-05-24 23:56:43.727206: train Epoch: [9][ 5/24]	Time  1.032 ( 1.828)	Data  0.001 ( 0.692)	Loss 1.8081e-01 (3.0996e-01) 
2023-05-24 23:56:45.578492: train Epoch: [9][ 6/24]	Time  1.851 ( 1.831)	Data  0.497 ( 0.664)	Loss 3.9591e-01 (3.2224e-01) 
2023-05-24 23:56:46.669895: train Epoch: [9][ 7/24]	Time  1.091 ( 1.739)	Data  0.001 ( 0.581)	Loss 2.1245e-01 (3.0851e-01) 
2023-05-24 23:56:48.178026: train Epoch: [9][ 8/24]	Time  1.508 ( 1.713)	Data  0.341 ( 0.554)	Loss 4.6888e-01 (3.2633e-01) 
2023-05-24 23:56:49.552077: train Epoch: [9][ 9/24]	Time  1.374 ( 1.679)	Data  0.001 ( 0.499)	Loss 3.4799e-01 (3.2850e-01) 
2023-05-24 23:56:50.851396: train Epoch: [9][10/24]	Time  1.299 ( 1.645)	Data  0.239 ( 0.475)	Loss 2.2634e-01 (3.1921e-01) 
2023-05-24 23:56:51.901061: train Epoch: [9][11/24]	Time  1.050 ( 1.595)	Data  0.001 ( 0.436)	Loss 1.7951e-01 (3.0757e-01) 
2023-05-24 23:56:53.500207: train Epoch: [9][12/24]	Time  1.599 ( 1.595)	Data  0.637 ( 0.451)	Loss 3.7768e-01 (3.1296e-01) 
2023-05-24 23:56:54.500234: train Epoch: [9][13/24]	Time  1.000 ( 1.553)	Data  0.001 ( 0.419)	Loss 2.8818e-01 (3.1119e-01) 
2023-05-24 23:56:56.510395: train Epoch: [9][14/24]	Time  2.010 ( 1.583)	Data  0.759 ( 0.442)	Loss 1.9974e-01 (3.0376e-01) 
2023-05-24 23:56:57.511092: train Epoch: [9][15/24]	Time  1.001 ( 1.547)	Data  0.001 ( 0.414)	Loss 2.8744e-01 (3.0274e-01) 
2023-05-24 23:56:59.483176: train Epoch: [9][16/24]	Time  1.972 ( 1.572)	Data  0.640 ( 0.428)	Loss 2.2179e-01 (2.9798e-01) 
2023-05-24 23:57:00.505283: train Epoch: [9][17/24]	Time  1.022 ( 1.541)	Data  0.001 ( 0.404)	Loss 2.1954e-01 (2.9362e-01) 
2023-05-24 23:57:02.383078: train Epoch: [9][18/24]	Time  1.878 ( 1.559)	Data  0.452 ( 0.406)	Loss 4.1188e-01 (2.9985e-01) 
2023-05-24 23:57:03.491645: train Epoch: [9][19/24]	Time  1.109 ( 1.537)	Data  0.001 ( 0.386)	Loss 2.4831e-01 (2.9727e-01) 
2023-05-24 23:57:05.010202: train Epoch: [9][20/24]	Time  1.519 ( 1.536)	Data  0.145 ( 0.375)	Loss 2.5025e-01 (2.9503e-01) 
2023-05-24 23:57:06.180249: train Epoch: [9][21/24]	Time  1.170 ( 1.519)	Data  0.001 ( 0.358)	Loss 2.0564e-01 (2.9097e-01) 
2023-05-24 23:57:07.378488: train Epoch: [9][22/24]	Time  1.198 ( 1.505)	Data  0.177 ( 0.350)	Loss 2.4043e-01 (2.8877e-01) 
2023-05-24 23:57:08.674147: train Epoch: [9][23/24]	Time  1.296 ( 1.496)	Data  0.001 ( 0.335)	Loss 1.7971e-01 (2.8423e-01) 
2023-05-24 23:57:08.727964: Train Epoch done in 35.96811621100642 s 
2023-05-24 23:57:12.027946: val Epoch: [9][0/9]	Time  2.245 ( 2.245)	Data  1.813 ( 1.813)	Loss 4.3702e-01 (4.3702e-01) 
2023-05-24 23:57:12.514370: val Epoch: [9][1/9]	Time  0.487 ( 1.366)	Data  0.002 ( 0.907)	Loss 2.4116e-01 (3.3909e-01) 
2023-05-24 23:57:13.368682: val Epoch: [9][2/9]	Time  0.854 ( 1.195)	Data  0.563 ( 0.793)	Loss 2.7512e-01 (3.1777e-01) 
2023-05-24 23:57:13.672689: val Epoch: [9][3/9]	Time  0.304 ( 0.972)	Data  0.001 ( 0.595)	Loss 2.5269e-01 (3.0150e-01) 
2023-05-24 23:57:14.754417: val Epoch: [9][4/9]	Time  1.082 ( 0.994)	Data  0.860 ( 0.648)	Loss 2.1146e-01 (2.8349e-01) 
2023-05-24 23:57:15.019619: val Epoch: [9][5/9]	Time  0.265 ( 0.873)	Data  0.001 ( 0.540)	Loss 2.2478e-01 (2.7371e-01) 
2023-05-24 23:57:16.176633: val Epoch: [9][6/9]	Time  1.157 ( 0.913)	Data  0.899 ( 0.591)	Loss 3.5094e-01 (2.8474e-01) 
2023-05-24 23:57:16.428014: val Epoch: [9][7/9]	Time  0.251 ( 0.831)	Data  0.001 ( 0.517)	Loss 3.6819e-01 (2.9517e-01) 
2023-05-24 23:57:17.330161: val Epoch: [9][8/9]	Time  0.902 ( 0.839)	Data  0.587 ( 0.525)	Loss 1.9892e-01 (2.8448e-01) 
2023-05-24 23:57:17.584255: Epoch 9 :Val : ['ET : 0.4488270580768585', 'TC : 0.6430814266204834', 'WT : 0.7431325912475586'] 
2023-05-24 23:57:17.589609: Epoch 9 :Val : ['ET : 0.4488270580768585', 'TC : 0.6430814266204834', 'WT : 0.7431325912475586'] 
2023-05-24 23:57:17.592032: Saving the model with DSC 0.6116803288459778 
2023-05-24 23:57:18.573487: Val epoch done in 9.845507910940796 s 
2023-05-24 23:57:18.581017: Batches per epoch:  24 
2023-05-24 23:57:22.934629: train Epoch: [10][ 0/24]	Time  4.353 ( 4.353)	Data  3.131 ( 3.131)	Loss 1.9246e-01 (1.9246e-01) 
2023-05-24 23:57:24.085063: train Epoch: [10][ 1/24]	Time  1.150 ( 2.752)	Data  0.001 ( 1.566)	Loss 1.8585e-01 (1.8915e-01) 
2023-05-24 23:57:25.435635: train Epoch: [10][ 2/24]	Time  1.351 ( 2.285)	Data  0.294 ( 1.142)	Loss 2.4077e-01 (2.0636e-01) 
2023-05-24 23:57:26.519954: train Epoch: [10][ 3/24]	Time  1.084 ( 1.985)	Data  0.001 ( 0.857)	Loss 3.4037e-01 (2.3986e-01) 
2023-05-24 23:57:28.218540: train Epoch: [10][ 4/24]	Time  1.699 ( 1.927)	Data  0.447 ( 0.775)	Loss 1.5014e-01 (2.2192e-01) 
2023-05-24 23:57:29.356641: train Epoch: [10][ 5/24]	Time  1.138 ( 1.796)	Data  0.001 ( 0.646)	Loss 3.9833e-01 (2.5132e-01) 
2023-05-24 23:57:30.733697: train Epoch: [10][ 6/24]	Time  1.377 ( 1.736)	Data  0.244 ( 0.588)	Loss 2.7304e-01 (2.5442e-01) 
2023-05-24 23:57:31.761670: train Epoch: [10][ 7/24]	Time  1.028 ( 1.648)	Data  0.002 ( 0.515)	Loss 3.3999e-01 (2.6512e-01) 
2023-05-24 23:57:33.605962: train Epoch: [10][ 8/24]	Time  1.844 ( 1.669)	Data  0.567 ( 0.521)	Loss 2.0465e-01 (2.5840e-01) 
2023-05-24 23:57:34.834775: train Epoch: [10][ 9/24]	Time  1.229 ( 1.625)	Data  0.001 ( 0.469)	Loss 4.0155e-01 (2.7271e-01) 
2023-05-24 23:57:35.993124: train Epoch: [10][10/24]	Time  1.158 ( 1.583)	Data  0.164 ( 0.441)	Loss 2.0681e-01 (2.6672e-01) 
2023-05-24 23:57:37.469367: train Epoch: [10][11/24]	Time  1.476 ( 1.574)	Data  0.425 ( 0.440)	Loss 2.3383e-01 (2.6398e-01) 
2023-05-24 23:57:38.807137: train Epoch: [10][12/24]	Time  1.338 ( 1.556)	Data  0.232 ( 0.424)	Loss 3.1128e-01 (2.6762e-01) 
2023-05-24 23:57:40.498127: train Epoch: [10][13/24]	Time  1.691 ( 1.565)	Data  0.473 ( 0.427)	Loss 2.1923e-01 (2.6416e-01) 
2023-05-24 23:57:41.755079: train Epoch: [10][14/24]	Time  1.257 ( 1.545)	Data  0.001 ( 0.399)	Loss 1.9044e-01 (2.5925e-01) 
2023-05-24 23:57:43.109454: train Epoch: [10][15/24]	Time  1.354 ( 1.533)	Data  0.325 ( 0.394)	Loss 2.4972e-01 (2.5865e-01) 
2023-05-24 23:57:44.181258: train Epoch: [10][16/24]	Time  1.072 ( 1.506)	Data  0.001 ( 0.371)	Loss 1.8989e-01 (2.5461e-01) 
2023-05-24 23:57:45.882730: train Epoch: [10][17/24]	Time  1.701 ( 1.517)	Data  0.648 ( 0.387)	Loss 4.1117e-01 (2.6331e-01) 
2023-05-24 23:57:46.944397: train Epoch: [10][18/24]	Time  1.062 ( 1.493)	Data  0.035 ( 0.368)	Loss 1.7814e-01 (2.5882e-01) 
2023-05-24 23:57:48.760277: train Epoch: [10][19/24]	Time  1.816 ( 1.509)	Data  0.792 ( 0.389)	Loss 2.7637e-01 (2.5970e-01) 
2023-05-24 23:57:49.833822: train Epoch: [10][20/24]	Time  1.074 ( 1.488)	Data  0.001 ( 0.371)	Loss 2.1163e-01 (2.5741e-01) 
2023-05-24 23:57:51.620053: train Epoch: [10][21/24]	Time  1.786 ( 1.502)	Data  0.654 ( 0.384)	Loss 2.8025e-01 (2.5845e-01) 
2023-05-24 23:57:52.601291: train Epoch: [10][22/24]	Time  0.981 ( 1.479)	Data  0.001 ( 0.367)	Loss 2.3439e-01 (2.5740e-01) 
2023-05-24 23:57:53.953442: train Epoch: [10][23/24]	Time  1.352 ( 1.474)	Data  0.242 ( 0.362)	Loss 2.8239e-01 (2.5844e-01) 
2023-05-24 23:57:54.071765: Train Epoch done in 35.49084426695481 s 
2023-05-24 23:57:57.256392: val Epoch: [10][0/9]	Time  2.172 ( 2.172)	Data  1.703 ( 1.703)	Loss 2.5091e-01 (2.5091e-01) 
2023-05-24 23:57:57.532188: val Epoch: [10][1/9]	Time  0.276 ( 1.224)	Data  0.002 ( 0.852)	Loss 4.1695e-01 (3.3393e-01) 
2023-05-24 23:57:58.318517: val Epoch: [10][2/9]	Time  0.786 ( 1.078)	Data  0.568 ( 0.758)	Loss 2.0611e-01 (2.9132e-01) 
2023-05-24 23:57:58.962654: val Epoch: [10][3/9]	Time  0.644 ( 0.970)	Data  0.152 ( 0.606)	Loss 2.0771e-01 (2.7042e-01) 
2023-05-24 23:57:59.962041: val Epoch: [10][4/9]	Time  0.999 ( 0.976)	Data  0.473 ( 0.580)	Loss 2.2183e-01 (2.6070e-01) 
2023-05-24 23:58:00.315480: val Epoch: [10][5/9]	Time  0.353 ( 0.872)	Data  0.062 ( 0.493)	Loss 3.8888e-01 (2.8206e-01) 
2023-05-24 23:58:01.223243: val Epoch: [10][6/9]	Time  0.908 ( 0.877)	Data  0.503 ( 0.495)	Loss 2.9074e-01 (2.8330e-01) 
2023-05-24 23:58:01.844282: val Epoch: [10][7/9]	Time  0.621 ( 0.845)	Data  0.294 ( 0.470)	Loss 3.6220e-01 (2.9317e-01) 
2023-05-24 23:58:02.297957: val Epoch: [10][8/9]	Time  0.454 ( 0.802)	Data  0.260 ( 0.446)	Loss 2.8199e-01 (2.9192e-01) 
2023-05-24 23:58:02.522876: Epoch 10 :Val : ['ET : 0.4247230291366577', 'TC : 0.6241185069084167', 'WT : 0.7353530526161194'] 
2023-05-24 23:58:02.523907: Epoch 10 :Val : ['ET : 0.4247230291366577', 'TC : 0.6241185069084167', 'WT : 0.7353530526161194'] 
2023-05-24 23:58:02.528362: Val epoch done in 8.456595957046375 s 
2023-05-24 23:58:02.536798: Batches per epoch:  24 
2023-05-24 23:58:07.410797: train Epoch: [11][ 0/24]	Time  4.873 ( 4.873)	Data  3.440 ( 3.440)	Loss 2.0735e-01 (2.0735e-01) 
2023-05-24 23:58:08.348496: train Epoch: [11][ 1/24]	Time  0.938 ( 2.906)	Data  0.001 ( 1.720)	Loss 2.2916e-01 (2.1825e-01) 
2023-05-24 23:58:09.979989: train Epoch: [11][ 2/24]	Time  1.631 ( 2.481)	Data  0.555 ( 1.332)	Loss 1.6878e-01 (2.0176e-01) 
2023-05-24 23:58:11.029622: train Epoch: [11][ 3/24]	Time  1.050 ( 2.123)	Data  0.001 ( 0.999)	Loss 2.3321e-01 (2.0962e-01) 
2023-05-24 23:58:12.858350: train Epoch: [11][ 4/24]	Time  1.829 ( 2.064)	Data  0.547 ( 0.909)	Loss 2.4947e-01 (2.1759e-01) 
2023-05-24 23:58:13.822371: train Epoch: [11][ 5/24]	Time  0.964 ( 1.881)	Data  0.001 ( 0.757)	Loss 2.4035e-01 (2.2139e-01) 
2023-05-24 23:58:15.938409: train Epoch: [11][ 6/24]	Time  2.116 ( 1.914)	Data  0.407 ( 0.707)	Loss 3.1435e-01 (2.3467e-01) 
2023-05-24 23:58:17.048096: train Epoch: [11][ 7/24]	Time  1.110 ( 1.814)	Data  0.001 ( 0.619)	Loss 3.8358e-01 (2.5328e-01) 
2023-05-24 23:58:17.986017: train Epoch: [11][ 8/24]	Time  0.938 ( 1.717)	Data  0.001 ( 0.550)	Loss 2.7264e-01 (2.5543e-01) 
2023-05-24 23:58:18.934922: train Epoch: [11][ 9/24]	Time  0.949 ( 1.640)	Data  0.001 ( 0.495)	Loss 1.7443e-01 (2.4733e-01) 
2023-05-24 23:58:20.567050: train Epoch: [11][10/24]	Time  1.632 ( 1.639)	Data  0.634 ( 0.508)	Loss 2.0613e-01 (2.4359e-01) 
2023-05-24 23:58:21.521831: train Epoch: [11][11/24]	Time  0.955 ( 1.582)	Data  0.001 ( 0.466)	Loss 2.6219e-01 (2.4514e-01) 
2023-05-24 23:58:23.369905: train Epoch: [11][12/24]	Time  1.848 ( 1.603)	Data  0.869 ( 0.497)	Loss 2.0630e-01 (2.4215e-01) 
2023-05-24 23:58:24.329026: train Epoch: [11][13/24]	Time  0.959 ( 1.557)	Data  0.001 ( 0.461)	Loss 2.1073e-01 (2.3991e-01) 
2023-05-24 23:58:26.154987: train Epoch: [11][14/24]	Time  1.826 ( 1.575)	Data  0.824 ( 0.486)	Loss 2.5742e-01 (2.4107e-01) 
2023-05-24 23:58:27.108520: train Epoch: [11][15/24]	Time  0.954 ( 1.536)	Data  0.001 ( 0.455)	Loss 3.6424e-01 (2.4877e-01) 
2023-05-24 23:58:28.802673: train Epoch: [11][16/24]	Time  1.694 ( 1.545)	Data  0.733 ( 0.472)	Loss 2.8966e-01 (2.5118e-01) 
2023-05-24 23:58:29.737213: train Epoch: [11][17/24]	Time  0.935 ( 1.511)	Data  0.001 ( 0.445)	Loss 2.4218e-01 (2.5068e-01) 
2023-05-24 23:58:31.355978: train Epoch: [11][18/24]	Time  1.619 ( 1.517)	Data  0.661 ( 0.457)	Loss 3.0590e-01 (2.5358e-01) 
2023-05-24 23:58:32.313189: train Epoch: [11][19/24]	Time  0.957 ( 1.489)	Data  0.001 ( 0.434)	Loss 3.6062e-01 (2.5893e-01) 
2023-05-24 23:58:33.968509: train Epoch: [11][20/24]	Time  1.655 ( 1.497)	Data  0.708 ( 0.447)	Loss 3.0040e-01 (2.6091e-01) 
2023-05-24 23:58:34.920758: train Epoch: [11][21/24]	Time  0.952 ( 1.472)	Data  0.001 ( 0.427)	Loss 2.2413e-01 (2.5924e-01) 
2023-05-24 23:58:37.479678: train Epoch: [11][22/24]	Time  2.559 ( 1.519)	Data  0.626 ( 0.435)	Loss 1.5409e-01 (2.5467e-01) 
2023-05-24 23:58:39.431561: train Epoch: [11][23/24]	Time  1.952 ( 1.537)	Data  0.001 ( 0.417)	Loss 4.7172e-01 (2.6371e-01) 
2023-05-24 23:58:39.482516: Train Epoch done in 36.94577704009134 s 
2023-05-24 23:58:42.664499: val Epoch: [11][0/9]	Time  2.234 ( 2.234)	Data  1.676 ( 1.676)	Loss 3.9086e-01 (3.9086e-01) 
2023-05-24 23:58:43.206301: val Epoch: [11][1/9]	Time  0.542 ( 1.388)	Data  0.002 ( 0.839)	Loss 2.4223e-01 (3.1655e-01) 
2023-05-24 23:58:43.966978: val Epoch: [11][2/9]	Time  0.761 ( 1.179)	Data  0.210 ( 0.629)	Loss 2.6121e-01 (2.9810e-01) 
2023-05-24 23:58:44.506023: val Epoch: [11][3/9]	Time  0.539 ( 1.019)	Data  0.001 ( 0.472)	Loss 2.0671e-01 (2.7525e-01) 
2023-05-24 23:58:45.197621: val Epoch: [11][4/9]	Time  0.692 ( 0.954)	Data  0.151 ( 0.408)	Loss 2.7168e-01 (2.7454e-01) 
2023-05-24 23:58:45.744275: val Epoch: [11][5/9]	Time  0.547 ( 0.886)	Data  0.001 ( 0.340)	Loss 2.2667e-01 (2.6656e-01) 
2023-05-24 23:58:46.473908: val Epoch: [11][6/9]	Time  0.730 ( 0.863)	Data  0.184 ( 0.318)	Loss 3.6541e-01 (2.8068e-01) 
2023-05-24 23:58:47.022336: val Epoch: [11][7/9]	Time  0.548 ( 0.824)	Data  0.001 ( 0.278)	Loss 4.5259e-01 (3.0217e-01) 
2023-05-24 23:58:47.577290: val Epoch: [11][8/9]	Time  0.555 ( 0.794)	Data  0.001 ( 0.247)	Loss 2.8747e-01 (3.0053e-01) 
2023-05-24 23:58:47.772460: Epoch 11 :Val : ['ET : 0.4342338740825653', 'TC : 0.6057014465332031', 'WT : 0.7042935490608215'] 
2023-05-24 23:58:47.773337: Epoch 11 :Val : ['ET : 0.4342338740825653', 'TC : 0.6057014465332031', 'WT : 0.7042935490608215'] 
2023-05-24 23:58:47.777371: Val epoch done in 8.294858624925837 s 
2023-05-24 23:58:47.788268: Batches per epoch:  24 
2023-05-24 23:58:52.763127: train Epoch: [12][ 0/24]	Time  4.974 ( 4.974)	Data  2.986 ( 2.986)	Loss 1.8261e-01 (1.8261e-01) 
2023-05-24 23:58:54.724883: train Epoch: [12][ 1/24]	Time  1.962 ( 3.468)	Data  0.001 ( 1.494)	Loss 3.6125e-01 (2.7193e-01) 
2023-05-24 23:58:56.670860: train Epoch: [12][ 2/24]	Time  1.946 ( 2.961)	Data  0.001 ( 0.996)	Loss 2.0470e-01 (2.4952e-01) 
2023-05-24 23:58:58.623549: train Epoch: [12][ 3/24]	Time  1.953 ( 2.709)	Data  0.001 ( 0.747)	Loss 2.5650e-01 (2.5126e-01) 
2023-05-24 23:59:00.562752: train Epoch: [12][ 4/24]	Time  1.939 ( 2.555)	Data  0.001 ( 0.598)	Loss 4.2883e-01 (2.8677e-01) 
2023-05-24 23:59:02.534080: train Epoch: [12][ 5/24]	Time  1.971 ( 2.458)	Data  0.001 ( 0.499)	Loss 3.4165e-01 (2.9592e-01) 
2023-05-24 23:59:04.490797: train Epoch: [12][ 6/24]	Time  1.957 ( 2.386)	Data  0.011 ( 0.429)	Loss 4.1766e-01 (3.1331e-01) 
2023-05-24 23:59:05.680459: train Epoch: [12][ 7/24]	Time  1.190 ( 2.236)	Data  0.001 ( 0.375)	Loss 4.2111e-01 (3.2679e-01) 
2023-05-24 23:59:06.661536: train Epoch: [12][ 8/24]	Time  0.981 ( 2.097)	Data  0.001 ( 0.334)	Loss 3.1324e-01 (3.2528e-01) 
2023-05-24 23:59:08.242379: train Epoch: [12][ 9/24]	Time  1.581 ( 2.045)	Data  0.001 ( 0.301)	Loss 1.6460e-01 (3.0921e-01) 
2023-05-24 23:59:09.320926: train Epoch: [12][10/24]	Time  1.079 ( 1.957)	Data  0.001 ( 0.273)	Loss 2.6449e-01 (3.0515e-01) 
2023-05-24 23:59:10.355200: train Epoch: [12][11/24]	Time  1.034 ( 1.881)	Data  0.001 ( 0.251)	Loss 2.2245e-01 (2.9826e-01) 
2023-05-24 23:59:11.371209: train Epoch: [12][12/24]	Time  1.016 ( 1.814)	Data  0.001 ( 0.231)	Loss 1.8924e-01 (2.8987e-01) 
2023-05-24 23:59:12.660015: train Epoch: [12][13/24]	Time  1.289 ( 1.777)	Data  0.001 ( 0.215)	Loss 1.6766e-01 (2.8114e-01) 
2023-05-24 23:59:13.671938: train Epoch: [12][14/24]	Time  1.012 ( 1.726)	Data  0.001 ( 0.201)	Loss 2.7042e-01 (2.8043e-01) 
2023-05-24 23:59:14.834392: train Epoch: [12][15/24]	Time  1.162 ( 1.690)	Data  0.001 ( 0.188)	Loss 2.2010e-01 (2.7666e-01) 
2023-05-24 23:59:16.143553: train Epoch: [12][16/24]	Time  1.309 ( 1.668)	Data  0.001 ( 0.177)	Loss 2.6873e-01 (2.7619e-01) 
2023-05-24 23:59:17.165112: train Epoch: [12][17/24]	Time  1.022 ( 1.632)	Data  0.001 ( 0.167)	Loss 2.1158e-01 (2.7260e-01) 
2023-05-24 23:59:18.912823: train Epoch: [12][18/24]	Time  1.748 ( 1.638)	Data  0.420 ( 0.181)	Loss 1.6697e-01 (2.6704e-01) 
2023-05-24 23:59:19.907468: train Epoch: [12][19/24]	Time  0.995 ( 1.606)	Data  0.001 ( 0.172)	Loss 2.0575e-01 (2.6398e-01) 
2023-05-24 23:59:21.764937: train Epoch: [12][20/24]	Time  1.857 ( 1.618)	Data  0.525 ( 0.189)	Loss 1.9076e-01 (2.6049e-01) 
2023-05-24 23:59:22.835249: train Epoch: [12][21/24]	Time  1.070 ( 1.593)	Data  0.001 ( 0.180)	Loss 2.3710e-01 (2.5943e-01) 
2023-05-24 23:59:24.141228: train Epoch: [12][22/24]	Time  1.306 ( 1.581)	Data  0.339 ( 0.187)	Loss 2.4555e-01 (2.5882e-01) 
2023-05-24 23:59:25.194182: train Epoch: [12][23/24]	Time  1.053 ( 1.559)	Data  0.001 ( 0.179)	Loss 1.9051e-01 (2.5598e-01) 
2023-05-24 23:59:25.282342: Train Epoch done in 37.494123760960065 s 
2023-05-24 23:59:28.483237: val Epoch: [12][0/9]	Time  2.234 ( 2.234)	Data  1.725 ( 1.725)	Loss 2.3445e-01 (2.3445e-01) 
2023-05-24 23:59:28.802783: val Epoch: [12][1/9]	Time  0.320 ( 1.277)	Data  0.002 ( 0.863)	Loss 2.5382e-01 (2.4413e-01) 
2023-05-24 23:59:29.889697: val Epoch: [12][2/9]	Time  1.087 ( 1.214)	Data  0.561 ( 0.762)	Loss 2.2586e-01 (2.3804e-01) 
2023-05-24 23:59:30.171213: val Epoch: [12][3/9]	Time  0.281 ( 0.981)	Data  0.001 ( 0.572)	Loss 2.8600e-01 (2.5003e-01) 
2023-05-24 23:59:31.205645: val Epoch: [12][4/9]	Time  1.034 ( 0.991)	Data  0.667 ( 0.591)	Loss 3.4500e-01 (2.6902e-01) 
2023-05-24 23:59:31.674038: val Epoch: [12][5/9]	Time  0.468 ( 0.904)	Data  0.001 ( 0.493)	Loss 2.7897e-01 (2.7068e-01) 
2023-05-24 23:59:32.601344: val Epoch: [12][6/9]	Time  0.927 ( 0.907)	Data  0.639 ( 0.513)	Loss 1.9107e-01 (2.5931e-01) 
2023-05-24 23:59:32.783675: val Epoch: [12][7/9]	Time  0.182 ( 0.817)	Data  0.001 ( 0.449)	Loss 4.2352e-01 (2.7983e-01) 
2023-05-24 23:59:33.662875: val Epoch: [12][8/9]	Time  0.879 ( 0.824)	Data  0.673 ( 0.474)	Loss 3.6331e-01 (2.8911e-01) 
2023-05-24 23:59:33.915935: Epoch 12 :Val : ['ET : 0.4329289197921753', 'TC : 0.6277675032615662', 'WT : 0.7344400882720947'] 
2023-05-24 23:59:33.919053: Epoch 12 :Val : ['ET : 0.4329289197921753', 'TC : 0.6277675032615662', 'WT : 0.7344400882720947'] 
2023-05-24 23:59:33.922177: Val epoch done in 8.639842914999463 s 
2023-05-24 23:59:33.929399: Batches per epoch:  24 
2023-05-24 23:59:38.561428: train Epoch: [13][ 0/24]	Time  4.632 ( 4.632)	Data  3.275 ( 3.275)	Loss 2.4861e-01 (2.4861e-01) 
2023-05-24 23:59:39.735919: train Epoch: [13][ 1/24]	Time  1.175 ( 2.903)	Data  0.001 ( 1.638)	Loss 4.8891e-01 (3.6876e-01) 
2023-05-24 23:59:41.077470: train Epoch: [13][ 2/24]	Time  1.342 ( 2.383)	Data  0.334 ( 1.203)	Loss 1.9448e-01 (3.1067e-01) 
2023-05-24 23:59:42.205893: train Epoch: [13][ 3/24]	Time  1.128 ( 2.069)	Data  0.001 ( 0.903)	Loss 1.8482e-01 (2.7920e-01) 
2023-05-24 23:59:43.914775: train Epoch: [13][ 4/24]	Time  1.709 ( 1.997)	Data  0.643 ( 0.851)	Loss 2.8768e-01 (2.8090e-01) 
2023-05-24 23:59:45.069223: train Epoch: [13][ 5/24]	Time  1.154 ( 1.857)	Data  0.001 ( 0.709)	Loss 2.7981e-01 (2.8072e-01) 
2023-05-24 23:59:46.686093: train Epoch: [13][ 6/24]	Time  1.617 ( 1.822)	Data  0.597 ( 0.693)	Loss 2.1424e-01 (2.7122e-01) 
2023-05-24 23:59:47.799881: train Epoch: [13][ 7/24]	Time  1.114 ( 1.734)	Data  0.001 ( 0.607)	Loss 4.3959e-01 (2.9227e-01) 
2023-05-24 23:59:49.608925: train Epoch: [13][ 8/24]	Time  1.809 ( 1.742)	Data  0.702 ( 0.617)	Loss 2.6239e-01 (2.8895e-01) 
2023-05-24 23:59:50.601384: train Epoch: [13][ 9/24]	Time  0.992 ( 1.667)	Data  0.001 ( 0.556)	Loss 3.4296e-01 (2.9435e-01) 
2023-05-24 23:59:52.574203: train Epoch: [13][10/24]	Time  1.973 ( 1.695)	Data  0.715 ( 0.570)	Loss 2.9562e-01 (2.9447e-01) 
2023-05-24 23:59:53.549505: train Epoch: [13][11/24]	Time  0.975 ( 1.635)	Data  0.001 ( 0.523)	Loss 2.0994e-01 (2.8742e-01) 
2023-05-24 23:59:55.439514: train Epoch: [13][12/24]	Time  1.890 ( 1.655)	Data  0.580 ( 0.527)	Loss 1.4348e-01 (2.7635e-01) 
2023-05-24 23:59:56.496847: train Epoch: [13][13/24]	Time  1.057 ( 1.612)	Data  0.001 ( 0.490)	Loss 2.0120e-01 (2.7098e-01) 
2023-05-24 23:59:57.561915: Stopping training loop, doing benchmark 
