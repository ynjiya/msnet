2023-05-26 02:25:16.354771: Created model 
2023-05-26 02:25:16.359220: total number of trainable parameters 86794044 
2023-05-26 02:25:19.610419: <bound method EDiceLoss_Val.metric of EDiceLoss_Val()> 
2023-05-26 02:25:19.618993: Train dataset number of batch: 387 
2023-05-26 02:25:19.619173: Val dataset number of batch: 72 
2023-05-26 02:25:19.619232: Bench Test dataset number of batch: 25 
2023-05-26 02:25:19.619365: start training now! 
2023-05-26 02:25:19.619431: Batches per epoch:  387 
2023-05-26 02:25:24.535786: train Epoch: [0][  0/387]	Time  4.916 ( 4.916)	Data  3.478 ( 3.478)	Loss 8.5170e-01 (8.5170e-01) 
2023-05-26 02:25:25.604146: train Epoch: [0][  1/387]	Time  1.068 ( 2.992)	Data  0.001 ( 1.739)	Loss 8.6711e-01 (8.5941e-01) 
2023-05-26 02:25:26.870511: train Epoch: [0][  2/387]	Time  1.266 ( 2.417)	Data  0.186 ( 1.222)	Loss 8.3614e-01 (8.5165e-01) 
2023-05-26 02:25:27.824725: train Epoch: [0][  3/387]	Time  0.954 ( 2.051)	Data  0.001 ( 0.917)	Loss 8.5388e-01 (8.5221e-01) 
2023-05-26 02:25:28.876806: train Epoch: [0][  4/387]	Time  1.052 ( 1.851)	Data  0.087 ( 0.751)	Loss 7.7841e-01 (8.3745e-01) 
2023-05-26 02:25:29.834970: train Epoch: [0][  5/387]	Time  0.958 ( 1.703)	Data  0.001 ( 0.626)	Loss 7.9513e-01 (8.3039e-01) 
2023-05-26 02:25:31.632384: train Epoch: [0][  6/387]	Time  1.797 ( 1.716)	Data  0.928 ( 0.669)	Loss 8.4315e-01 (8.3222e-01) 
2023-05-26 02:25:32.562225: train Epoch: [0][  7/387]	Time  0.930 ( 1.618)	Data  0.001 ( 0.585)	Loss 7.9184e-01 (8.2717e-01) 
2023-05-26 02:25:34.435237: train Epoch: [0][  8/387]	Time  1.873 ( 1.646)	Data  0.941 ( 0.625)	Loss 8.5066e-01 (8.2978e-01) 
2023-05-26 02:25:35.427579: train Epoch: [0][  9/387]	Time  0.992 ( 1.581)	Data  0.001 ( 0.563)	Loss 8.5318e-01 (8.3212e-01) 
2023-05-26 02:25:37.562569: train Epoch: [0][ 10/387]	Time  2.135 ( 1.631)	Data  0.983 ( 0.601)	Loss 7.4346e-01 (8.2406e-01) 
2023-05-26 02:25:38.509341: train Epoch: [0][ 11/387]	Time  0.947 ( 1.574)	Data  0.001 ( 0.551)	Loss 8.3381e-01 (8.2487e-01) 
2023-05-26 02:25:40.318984: train Epoch: [0][ 12/387]	Time  1.810 ( 1.592)	Data  0.849 ( 0.574)	Loss 7.2967e-01 (8.1755e-01) 
2023-05-26 02:25:41.273955: train Epoch: [0][ 13/387]	Time  0.955 ( 1.547)	Data  0.001 ( 0.533)	Loss 7.3189e-01 (8.1143e-01) 
2023-05-26 02:25:43.308375: train Epoch: [0][ 14/387]	Time  2.034 ( 1.579)	Data  0.986 ( 0.563)	Loss 7.8699e-01 (8.0980e-01) 
2023-05-26 02:25:44.446617: train Epoch: [0][ 15/387]	Time  1.138 ( 1.552)	Data  0.001 ( 0.528)	Loss 8.0992e-01 (8.0981e-01) 
2023-05-26 02:25:45.990320: train Epoch: [0][ 16/387]	Time  1.544 ( 1.551)	Data  0.494 ( 0.526)	Loss 7.2525e-01 (8.0483e-01) 
2023-05-26 02:25:46.927394: train Epoch: [0][ 17/387]	Time  0.937 ( 1.517)	Data  0.001 ( 0.497)	Loss 7.2893e-01 (8.0062e-01) 
2023-05-26 02:25:48.682838: train Epoch: [0][ 18/387]	Time  1.755 ( 1.530)	Data  0.785 ( 0.512)	Loss 7.3731e-01 (7.9729e-01) 
2023-05-26 02:25:49.612670: train Epoch: [0][ 19/387]	Time  0.930 ( 1.500)	Data  0.001 ( 0.486)	Loss 7.5175e-01 (7.9501e-01) 
2023-05-26 02:25:51.435104: train Epoch: [0][ 20/387]	Time  1.822 ( 1.515)	Data  0.862 ( 0.504)	Loss 7.1489e-01 (7.9119e-01) 
2023-05-26 02:25:52.497252: train Epoch: [0][ 21/387]	Time  1.062 ( 1.494)	Data  0.001 ( 0.481)	Loss 7.4656e-01 (7.8916e-01) 
2023-05-26 02:25:54.311155: train Epoch: [0][ 22/387]	Time  1.814 ( 1.508)	Data  0.895 ( 0.499)	Loss 7.6986e-01 (7.8833e-01) 
2023-05-26 02:25:55.259050: train Epoch: [0][ 23/387]	Time  0.948 ( 1.485)	Data  0.001 ( 0.479)	Loss 7.0545e-01 (7.8487e-01) 
